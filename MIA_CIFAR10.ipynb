{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIA_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taoye9/Experiments-MIA/blob/master/MIA_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns5C9HOC8VUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "bz = 32 # batch size\n",
        "ration_attack_train = 0.7 # for attack mode training and testing \n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9XjVHOY8kHp",
        "colab_type": "text"
      },
      "source": [
        "# experiment 1: directly membership detection on target model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSMxpl9G8iZs",
        "colab_type": "code",
        "outputId": "f1a4d332-127d-4acb-c7b3-7f785a9184cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data.dataset import ConcatDataset, random_split\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# prepare data\n",
        "\n",
        "# Testset is used for eval of all model\n",
        "testloader =  torch.utils.data.DataLoader(testset, batch_size=bz,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "num_target_train = 15000\n",
        "num_shadow_train = len(trainset)-num_target_train\n",
        "print('target train dataset size is:{}\\tshadow training set size is: {}'.format(num_target_train, num_shadow_train))\n",
        "\n",
        "Dataset_target_train, Dataset_shadow_train = random_split(trainset, [num_target_train, num_shadow_train])\n",
        "\n",
        "loader_target_train = torch.utils.data.DataLoader(Dataset_target_train, batch_size=bz,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 71022992.66it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "target train dataset size is:15000\tshadow training set size is: 35000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovsI6RPQ9xnZ",
        "colab_type": "text"
      },
      "source": [
        "## define target model \n",
        "from pytorch tutorial: https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9vSZHew9tjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#LeNet \n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1satT8c-KUT",
        "colab_type": "code",
        "outputId": "6d600533-c2af-403e-d80e-28a72c0e6f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print('dev: {} \\n'.format(dev))\n",
        "    \n",
        "net = Net()\n",
        "net = net.to(dev)\n",
        "if dev == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "    \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40, 70], gamma=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev: cuda \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHJBvB-q-qmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, trainData=loader_target_train, log_interval=200):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    scheduler.step(epoch)\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader_target_train):\n",
        "        data, targets = inputs.to(dev), targets.to(dev)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, accuracy: {:.6f}: {}/{}'.format(epoch, batch_idx * len(data), len(loader_target_train.dataset),100. * batch_idx / len(loader_target_train), train_loss/(batch_idx+1), 100.*correct/total, correct, total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfen1wwL-rbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(epoch, testData=testloader):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(dev), targets.to(dev)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "    print(epoch, len(testloader), 'TESTING: Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEcugL7e-tvm",
        "colab_type": "code",
        "outputId": "573a38ec-072e-4bc7-b045-07cc6abd8677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+75):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Train Epoch: 0 [0/15000 (0%)]\tLoss: 2.296122, accuracy: 12.500000: 4/32\n",
            "Train Epoch: 0 [6400/15000 (43%)]\tLoss: 2.044741, accuracy: 23.849502: 1534/6432\n",
            "Train Epoch: 0 [12800/15000 (85%)]\tLoss: 1.892108, accuracy: 30.042082: 3855/12832\n",
            "0 313 TESTING: Loss: 1.636 | Acc: 39.980% (3998/10000)\n",
            "\n",
            "Epoch: 1\n",
            "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.410623, accuracy: 43.750000: 14/32\n",
            "Train Epoch: 1 [6400/15000 (43%)]\tLoss: 1.581895, accuracy: 41.806592: 2689/6432\n",
            "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.560453, accuracy: 43.321384: 5559/12832\n",
            "1 313 TESTING: Loss: 1.495 | Acc: 45.960% (4596/10000)\n",
            "\n",
            "Epoch: 2\n",
            "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.346161, accuracy: 50.000000: 16/32\n",
            "Train Epoch: 2 [6400/15000 (43%)]\tLoss: 1.470775, accuracy: 47.481343: 3054/6432\n",
            "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.451925, accuracy: 47.740025: 6126/12832\n",
            "2 313 TESTING: Loss: 1.448 | Acc: 47.100% (4710/10000)\n",
            "\n",
            "Epoch: 3\n",
            "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.304267, accuracy: 43.750000: 14/32\n",
            "Train Epoch: 3 [6400/15000 (43%)]\tLoss: 1.380975, accuracy: 50.233209: 3231/6432\n",
            "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.375574, accuracy: 50.600062: 6493/12832\n",
            "3 313 TESTING: Loss: 1.392 | Acc: 49.660% (4966/10000)\n",
            "\n",
            "Epoch: 4\n",
            "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.063289, accuracy: 65.625000: 21/32\n",
            "Train Epoch: 4 [6400/15000 (43%)]\tLoss: 1.284583, accuracy: 53.871269: 3465/6432\n",
            "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.297184, accuracy: 53.631546: 6882/12832\n",
            "4 313 TESTING: Loss: 1.340 | Acc: 51.440% (5144/10000)\n",
            "\n",
            "Epoch: 5\n",
            "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.369781, accuracy: 59.375000: 19/32\n",
            "Train Epoch: 5 [6400/15000 (43%)]\tLoss: 1.251426, accuracy: 55.457090: 3567/6432\n",
            "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.246072, accuracy: 55.556421: 7129/12832\n",
            "5 313 TESTING: Loss: 1.315 | Acc: 52.770% (5277/10000)\n",
            "\n",
            "Epoch: 6\n",
            "Train Epoch: 6 [0/15000 (0%)]\tLoss: 1.002751, accuracy: 56.250000: 18/32\n",
            "Train Epoch: 6 [6400/15000 (43%)]\tLoss: 1.185419, accuracy: 57.944652: 3727/6432\n",
            "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 1.184654, accuracy: 57.754052: 7411/12832\n",
            "6 313 TESTING: Loss: 1.319 | Acc: 52.570% (5257/10000)\n",
            "\n",
            "Epoch: 7\n",
            "Train Epoch: 7 [0/15000 (0%)]\tLoss: 1.114177, accuracy: 65.625000: 21/32\n",
            "Train Epoch: 7 [6400/15000 (43%)]\tLoss: 1.116931, accuracy: 60.199005: 3872/6432\n",
            "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 1.124266, accuracy: 60.099751: 7712/12832\n",
            "7 313 TESTING: Loss: 1.287 | Acc: 54.120% (5412/10000)\n",
            "\n",
            "Epoch: 8\n",
            "Train Epoch: 8 [0/15000 (0%)]\tLoss: 1.290747, accuracy: 53.125000: 17/32\n",
            "Train Epoch: 8 [6400/15000 (43%)]\tLoss: 1.056104, accuracy: 62.655473: 4030/6432\n",
            "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 1.076729, accuracy: 61.642768: 7910/12832\n",
            "8 313 TESTING: Loss: 1.307 | Acc: 53.970% (5397/10000)\n",
            "\n",
            "Epoch: 9\n",
            "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.734998, accuracy: 84.375000: 27/32\n",
            "Train Epoch: 9 [6400/15000 (43%)]\tLoss: 1.017956, accuracy: 63.277363: 4070/6432\n",
            "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 1.031171, accuracy: 63.162406: 8105/12832\n",
            "9 313 TESTING: Loss: 1.342 | Acc: 53.500% (5350/10000)\n",
            "\n",
            "Epoch: 10\n",
            "Train Epoch: 10 [0/15000 (0%)]\tLoss: 1.205778, accuracy: 56.250000: 18/32\n",
            "Train Epoch: 10 [6400/15000 (43%)]\tLoss: 0.963114, accuracy: 65.531716: 4215/6432\n",
            "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.982325, accuracy: 64.837905: 8320/12832\n",
            "10 313 TESTING: Loss: 1.319 | Acc: 54.620% (5462/10000)\n",
            "\n",
            "Epoch: 11\n",
            "Train Epoch: 11 [0/15000 (0%)]\tLoss: 0.719377, accuracy: 75.000000: 24/32\n",
            "Train Epoch: 11 [6400/15000 (43%)]\tLoss: 0.939546, accuracy: 66.759950: 4294/6432\n",
            "Train Epoch: 11 [12800/15000 (85%)]\tLoss: 0.939149, accuracy: 66.591334: 8545/12832\n",
            "11 313 TESTING: Loss: 1.318 | Acc: 55.180% (5518/10000)\n",
            "\n",
            "Epoch: 12\n",
            "Train Epoch: 12 [0/15000 (0%)]\tLoss: 0.788208, accuracy: 68.750000: 22/32\n",
            "Train Epoch: 12 [6400/15000 (43%)]\tLoss: 0.867859, accuracy: 69.667289: 4481/6432\n",
            "Train Epoch: 12 [12800/15000 (85%)]\tLoss: 0.885473, accuracy: 68.594140: 8802/12832\n",
            "12 313 TESTING: Loss: 1.359 | Acc: 54.170% (5417/10000)\n",
            "\n",
            "Epoch: 13\n",
            "Train Epoch: 13 [0/15000 (0%)]\tLoss: 0.862793, accuracy: 68.750000: 22/32\n",
            "Train Epoch: 13 [6400/15000 (43%)]\tLoss: 0.819582, accuracy: 71.035448: 4569/6432\n",
            "Train Epoch: 13 [12800/15000 (85%)]\tLoss: 0.848092, accuracy: 69.731920: 8948/12832\n",
            "13 313 TESTING: Loss: 1.338 | Acc: 55.240% (5524/10000)\n",
            "\n",
            "Epoch: 14\n",
            "Train Epoch: 14 [0/15000 (0%)]\tLoss: 0.933588, accuracy: 62.500000: 20/32\n",
            "Train Epoch: 14 [6400/15000 (43%)]\tLoss: 0.766829, accuracy: 73.103234: 4702/6432\n",
            "Train Epoch: 14 [12800/15000 (85%)]\tLoss: 0.794120, accuracy: 71.929551: 9230/12832\n",
            "14 313 TESTING: Loss: 1.376 | Acc: 54.930% (5493/10000)\n",
            "\n",
            "Epoch: 15\n",
            "Train Epoch: 15 [0/15000 (0%)]\tLoss: 0.627164, accuracy: 84.375000: 27/32\n",
            "Train Epoch: 15 [6400/15000 (43%)]\tLoss: 0.727018, accuracy: 74.657960: 4802/6432\n",
            "Train Epoch: 15 [12800/15000 (85%)]\tLoss: 0.752829, accuracy: 73.082918: 9378/12832\n",
            "15 313 TESTING: Loss: 1.442 | Acc: 54.010% (5401/10000)\n",
            "\n",
            "Epoch: 16\n",
            "Train Epoch: 16 [0/15000 (0%)]\tLoss: 0.865464, accuracy: 68.750000: 22/32\n",
            "Train Epoch: 16 [6400/15000 (43%)]\tLoss: 0.694705, accuracy: 75.481965: 4855/6432\n",
            "Train Epoch: 16 [12800/15000 (85%)]\tLoss: 0.715554, accuracy: 74.376559: 9544/12832\n",
            "16 313 TESTING: Loss: 1.491 | Acc: 54.230% (5423/10000)\n",
            "\n",
            "Epoch: 17\n",
            "Train Epoch: 17 [0/15000 (0%)]\tLoss: 0.576466, accuracy: 81.250000: 26/32\n",
            "Train Epoch: 17 [6400/15000 (43%)]\tLoss: 0.633176, accuracy: 77.907338: 5011/6432\n",
            "Train Epoch: 17 [12800/15000 (85%)]\tLoss: 0.654711, accuracy: 76.870324: 9864/12832\n",
            "17 313 TESTING: Loss: 1.562 | Acc: 54.010% (5401/10000)\n",
            "\n",
            "Epoch: 18\n",
            "Train Epoch: 18 [0/15000 (0%)]\tLoss: 0.405727, accuracy: 90.625000: 29/32\n",
            "Train Epoch: 18 [6400/15000 (43%)]\tLoss: 0.608529, accuracy: 78.638060: 5058/6432\n",
            "Train Epoch: 18 [12800/15000 (85%)]\tLoss: 0.632145, accuracy: 77.524938: 9948/12832\n",
            "18 313 TESTING: Loss: 1.609 | Acc: 54.650% (5465/10000)\n",
            "\n",
            "Epoch: 19\n",
            "Train Epoch: 19 [0/15000 (0%)]\tLoss: 0.519294, accuracy: 81.250000: 26/32\n",
            "Train Epoch: 19 [6400/15000 (43%)]\tLoss: 0.560383, accuracy: 80.659204: 5188/6432\n",
            "Train Epoch: 19 [12800/15000 (85%)]\tLoss: 0.587936, accuracy: 79.293953: 10175/12832\n",
            "19 313 TESTING: Loss: 1.728 | Acc: 52.970% (5297/10000)\n",
            "\n",
            "Epoch: 20\n",
            "Train Epoch: 20 [0/15000 (0%)]\tLoss: 0.530165, accuracy: 84.375000: 27/32\n",
            "Train Epoch: 20 [6400/15000 (43%)]\tLoss: 0.520906, accuracy: 81.638682: 5251/6432\n",
            "Train Epoch: 20 [12800/15000 (85%)]\tLoss: 0.545804, accuracy: 80.579800: 10340/12832\n",
            "20 313 TESTING: Loss: 1.700 | Acc: 52.730% (5273/10000)\n",
            "\n",
            "Epoch: 21\n",
            "Train Epoch: 21 [0/15000 (0%)]\tLoss: 0.442788, accuracy: 81.250000: 26/32\n",
            "Train Epoch: 21 [6400/15000 (43%)]\tLoss: 0.483504, accuracy: 82.991294: 5338/6432\n",
            "Train Epoch: 21 [12800/15000 (85%)]\tLoss: 0.509703, accuracy: 81.725374: 10487/12832\n",
            "21 313 TESTING: Loss: 1.827 | Acc: 52.850% (5285/10000)\n",
            "\n",
            "Epoch: 22\n",
            "Train Epoch: 22 [0/15000 (0%)]\tLoss: 0.306477, accuracy: 93.750000: 30/32\n",
            "Train Epoch: 22 [6400/15000 (43%)]\tLoss: 0.449694, accuracy: 84.654851: 5445/6432\n",
            "Train Epoch: 22 [12800/15000 (85%)]\tLoss: 0.469689, accuracy: 83.283978: 10687/12832\n",
            "22 313 TESTING: Loss: 1.962 | Acc: 52.160% (5216/10000)\n",
            "\n",
            "Epoch: 23\n",
            "Train Epoch: 23 [0/15000 (0%)]\tLoss: 0.292292, accuracy: 87.500000: 28/32\n",
            "Train Epoch: 23 [6400/15000 (43%)]\tLoss: 0.434836, accuracy: 84.328358: 5424/6432\n",
            "Train Epoch: 23 [12800/15000 (85%)]\tLoss: 0.455156, accuracy: 83.595698: 10727/12832\n",
            "23 313 TESTING: Loss: 1.907 | Acc: 52.710% (5271/10000)\n",
            "\n",
            "Epoch: 24\n",
            "Train Epoch: 24 [0/15000 (0%)]\tLoss: 0.438433, accuracy: 78.125000: 25/32\n",
            "Train Epoch: 24 [6400/15000 (43%)]\tLoss: 0.395365, accuracy: 86.318408: 5552/6432\n",
            "Train Epoch: 24 [12800/15000 (85%)]\tLoss: 0.418312, accuracy: 85.177681: 10930/12832\n",
            "24 313 TESTING: Loss: 2.073 | Acc: 52.760% (5276/10000)\n",
            "\n",
            "Epoch: 25\n",
            "Train Epoch: 25 [0/15000 (0%)]\tLoss: 0.359437, accuracy: 90.625000: 29/32\n",
            "Train Epoch: 25 [6400/15000 (43%)]\tLoss: 0.376432, accuracy: 86.738184: 5579/6432\n",
            "Train Epoch: 25 [12800/15000 (85%)]\tLoss: 0.382481, accuracy: 86.416771: 11089/12832\n",
            "25 313 TESTING: Loss: 2.153 | Acc: 52.100% (5210/10000)\n",
            "\n",
            "Epoch: 26\n",
            "Train Epoch: 26 [0/15000 (0%)]\tLoss: 0.381410, accuracy: 93.750000: 30/32\n",
            "Train Epoch: 26 [6400/15000 (43%)]\tLoss: 0.343824, accuracy: 88.199627: 5673/6432\n",
            "Train Epoch: 26 [12800/15000 (85%)]\tLoss: 0.367869, accuracy: 87.009040: 11165/12832\n",
            "26 313 TESTING: Loss: 2.266 | Acc: 51.700% (5170/10000)\n",
            "\n",
            "Epoch: 27\n",
            "Train Epoch: 27 [0/15000 (0%)]\tLoss: 0.204185, accuracy: 93.750000: 30/32\n",
            "Train Epoch: 27 [6400/15000 (43%)]\tLoss: 0.311940, accuracy: 89.241294: 5740/6432\n",
            "Train Epoch: 27 [12800/15000 (85%)]\tLoss: 0.331526, accuracy: 88.528678: 11360/12832\n",
            "27 313 TESTING: Loss: 2.434 | Acc: 52.370% (5237/10000)\n",
            "\n",
            "Epoch: 28\n",
            "Train Epoch: 28 [0/15000 (0%)]\tLoss: 0.551780, accuracy: 84.375000: 27/32\n",
            "Train Epoch: 28 [6400/15000 (43%)]\tLoss: 0.296191, accuracy: 90.018657: 5790/6432\n",
            "Train Epoch: 28 [12800/15000 (85%)]\tLoss: 0.324828, accuracy: 88.606608: 11370/12832\n",
            "28 313 TESTING: Loss: 2.453 | Acc: 52.080% (5208/10000)\n",
            "\n",
            "Epoch: 29\n",
            "Train Epoch: 29 [0/15000 (0%)]\tLoss: 0.111535, accuracy: 96.875000: 31/32\n",
            "Train Epoch: 29 [6400/15000 (43%)]\tLoss: 0.271967, accuracy: 90.625000: 5829/6432\n",
            "Train Epoch: 29 [12800/15000 (85%)]\tLoss: 0.295300, accuracy: 89.588529: 11496/12832\n",
            "29 313 TESTING: Loss: 2.660 | Acc: 51.300% (5130/10000)\n",
            "\n",
            "Epoch: 30\n",
            "Train Epoch: 30 [0/15000 (0%)]\tLoss: 0.289474, accuracy: 93.750000: 30/32\n",
            "Train Epoch: 30 [6400/15000 (43%)]\tLoss: 0.250877, accuracy: 91.604478: 5892/6432\n",
            "Train Epoch: 30 [12800/15000 (85%)]\tLoss: 0.280398, accuracy: 90.297693: 11587/12832\n",
            "30 313 TESTING: Loss: 2.606 | Acc: 51.760% (5176/10000)\n",
            "\n",
            "Epoch: 31\n",
            "Train Epoch: 31 [0/15000 (0%)]\tLoss: 0.137512, accuracy: 96.875000: 31/32\n",
            "Train Epoch: 31 [6400/15000 (43%)]\tLoss: 0.235243, accuracy: 92.039801: 5920/6432\n",
            "Train Epoch: 31 [12800/15000 (85%)]\tLoss: 0.263384, accuracy: 90.632793: 11630/12832\n",
            "31 313 TESTING: Loss: 2.851 | Acc: 51.620% (5162/10000)\n",
            "\n",
            "Epoch: 32\n",
            "Train Epoch: 32 [0/15000 (0%)]\tLoss: 0.086655, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 32 [6400/15000 (43%)]\tLoss: 0.224647, accuracy: 92.101990: 5924/6432\n",
            "Train Epoch: 32 [12800/15000 (85%)]\tLoss: 0.255393, accuracy: 90.819825: 11654/12832\n",
            "32 313 TESTING: Loss: 2.953 | Acc: 51.580% (5158/10000)\n",
            "\n",
            "Epoch: 33\n",
            "Train Epoch: 33 [0/15000 (0%)]\tLoss: 0.289074, accuracy: 87.500000: 28/32\n",
            "Train Epoch: 33 [6400/15000 (43%)]\tLoss: 0.208219, accuracy: 92.646144: 5959/6432\n",
            "Train Epoch: 33 [12800/15000 (85%)]\tLoss: 0.227845, accuracy: 92.043329: 11811/12832\n",
            "33 313 TESTING: Loss: 2.990 | Acc: 51.150% (5115/10000)\n",
            "\n",
            "Epoch: 34\n",
            "Train Epoch: 34 [0/15000 (0%)]\tLoss: 0.248723, accuracy: 87.500000: 28/32\n",
            "Train Epoch: 34 [6400/15000 (43%)]\tLoss: 0.197806, accuracy: 93.283582: 6000/6432\n",
            "Train Epoch: 34 [12800/15000 (85%)]\tLoss: 0.219455, accuracy: 92.487531: 11868/12832\n",
            "34 313 TESTING: Loss: 3.201 | Acc: 51.370% (5137/10000)\n",
            "\n",
            "Epoch: 35\n",
            "Train Epoch: 35 [0/15000 (0%)]\tLoss: 0.110552, accuracy: 96.875000: 31/32\n",
            "Train Epoch: 35 [6400/15000 (43%)]\tLoss: 0.178110, accuracy: 93.796642: 6033/6432\n",
            "Train Epoch: 35 [12800/15000 (85%)]\tLoss: 0.207519, accuracy: 92.658978: 11890/12832\n",
            "35 313 TESTING: Loss: 3.312 | Acc: 50.680% (5068/10000)\n",
            "\n",
            "Epoch: 36\n",
            "Train Epoch: 36 [0/15000 (0%)]\tLoss: 0.311813, accuracy: 81.250000: 26/32\n",
            "Train Epoch: 36 [6400/15000 (43%)]\tLoss: 0.220781, accuracy: 92.599502: 5956/6432\n",
            "Train Epoch: 36 [12800/15000 (85%)]\tLoss: 0.220844, accuracy: 92.464152: 11865/12832\n",
            "36 313 TESTING: Loss: 3.339 | Acc: 51.580% (5158/10000)\n",
            "\n",
            "Epoch: 37\n",
            "Train Epoch: 37 [0/15000 (0%)]\tLoss: 0.161626, accuracy: 93.750000: 30/32\n",
            "Train Epoch: 37 [6400/15000 (43%)]\tLoss: 0.170193, accuracy: 94.200871: 6059/6432\n",
            "Train Epoch: 37 [12800/15000 (85%)]\tLoss: 0.189842, accuracy: 93.477244: 11995/12832\n",
            "37 313 TESTING: Loss: 3.520 | Acc: 51.210% (5121/10000)\n",
            "\n",
            "Epoch: 38\n",
            "Train Epoch: 38 [0/15000 (0%)]\tLoss: 0.130879, accuracy: 96.875000: 31/32\n",
            "Train Epoch: 38 [6400/15000 (43%)]\tLoss: 0.146495, accuracy: 95.087065: 6116/6432\n",
            "Train Epoch: 38 [12800/15000 (85%)]\tLoss: 0.171692, accuracy: 93.898067: 12049/12832\n",
            "38 313 TESTING: Loss: 3.657 | Acc: 50.770% (5077/10000)\n",
            "\n",
            "Epoch: 39\n",
            "Train Epoch: 39 [0/15000 (0%)]\tLoss: 0.170695, accuracy: 90.625000: 29/32\n",
            "Train Epoch: 39 [6400/15000 (43%)]\tLoss: 0.125004, accuracy: 96.019900: 6176/6432\n",
            "Train Epoch: 39 [12800/15000 (85%)]\tLoss: 0.160569, accuracy: 94.482544: 12124/12832\n",
            "39 313 TESTING: Loss: 3.727 | Acc: 51.010% (5101/10000)\n",
            "\n",
            "Epoch: 40\n",
            "Train Epoch: 40 [0/15000 (0%)]\tLoss: 0.142232, accuracy: 90.625000: 29/32\n",
            "Train Epoch: 40 [6400/15000 (43%)]\tLoss: 0.088332, accuracy: 97.543532: 6274/6432\n",
            "Train Epoch: 40 [12800/15000 (85%)]\tLoss: 0.079671, accuracy: 97.849127: 12556/12832\n",
            "40 313 TESTING: Loss: 3.836 | Acc: 52.090% (5209/10000)\n",
            "\n",
            "Epoch: 41\n",
            "Train Epoch: 41 [0/15000 (0%)]\tLoss: 0.010243, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 41 [6400/15000 (43%)]\tLoss: 0.042710, accuracy: 99.393657: 6393/6432\n",
            "Train Epoch: 41 [12800/15000 (85%)]\tLoss: 0.044883, accuracy: 99.259663: 12737/12832\n",
            "41 313 TESTING: Loss: 4.041 | Acc: 51.810% (5181/10000)\n",
            "\n",
            "Epoch: 42\n",
            "Train Epoch: 42 [0/15000 (0%)]\tLoss: 0.058729, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 42 [6400/15000 (43%)]\tLoss: 0.033181, accuracy: 99.518035: 6401/6432\n",
            "Train Epoch: 42 [12800/15000 (85%)]\tLoss: 0.035520, accuracy: 99.415524: 12757/12832\n",
            "42 313 TESTING: Loss: 4.230 | Acc: 51.810% (5181/10000)\n",
            "\n",
            "Epoch: 43\n",
            "Train Epoch: 43 [0/15000 (0%)]\tLoss: 0.023412, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 43 [6400/15000 (43%)]\tLoss: 0.030197, accuracy: 99.564677: 6404/6432\n",
            "Train Epoch: 43 [12800/15000 (85%)]\tLoss: 0.031321, accuracy: 99.501247: 12768/12832\n",
            "43 313 TESTING: Loss: 4.377 | Acc: 51.720% (5172/10000)\n",
            "\n",
            "Epoch: 44\n",
            "Train Epoch: 44 [0/15000 (0%)]\tLoss: 0.011439, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 44 [6400/15000 (43%)]\tLoss: 0.026673, accuracy: 99.595771: 6406/6432\n",
            "Train Epoch: 44 [12800/15000 (85%)]\tLoss: 0.033392, accuracy: 99.290835: 12741/12832\n",
            "44 313 TESTING: Loss: 4.581 | Acc: 51.010% (5101/10000)\n",
            "\n",
            "Epoch: 45\n",
            "Train Epoch: 45 [0/15000 (0%)]\tLoss: 0.069027, accuracy: 96.875000: 31/32\n",
            "Train Epoch: 45 [6400/15000 (43%)]\tLoss: 0.050206, accuracy: 98.662935: 6346/6432\n",
            "Train Epoch: 45 [12800/15000 (85%)]\tLoss: 0.066401, accuracy: 97.989401: 12574/12832\n",
            "45 313 TESTING: Loss: 4.615 | Acc: 51.190% (5119/10000)\n",
            "\n",
            "Epoch: 46\n",
            "Train Epoch: 46 [0/15000 (0%)]\tLoss: 0.071830, accuracy: 96.875000: 31/32\n",
            "Train Epoch: 46 [6400/15000 (43%)]\tLoss: 0.065854, accuracy: 97.901119: 6297/6432\n",
            "Train Epoch: 46 [12800/15000 (85%)]\tLoss: 0.060598, accuracy: 98.230985: 12605/12832\n",
            "46 313 TESTING: Loss: 4.787 | Acc: 51.680% (5168/10000)\n",
            "\n",
            "Epoch: 47\n",
            "Train Epoch: 47 [0/15000 (0%)]\tLoss: 0.021433, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 47 [6400/15000 (43%)]\tLoss: 0.043263, accuracy: 98.896144: 6361/6432\n",
            "Train Epoch: 47 [12800/15000 (85%)]\tLoss: 0.043164, accuracy: 98.916771: 12693/12832\n",
            "47 313 TESTING: Loss: 4.771 | Acc: 51.350% (5135/10000)\n",
            "\n",
            "Epoch: 48\n",
            "Train Epoch: 48 [0/15000 (0%)]\tLoss: 0.017035, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 48 [6400/15000 (43%)]\tLoss: 0.020500, accuracy: 99.797886: 6419/6432\n",
            "Train Epoch: 48 [12800/15000 (85%)]\tLoss: 0.021951, accuracy: 99.727244: 12797/12832\n",
            "48 313 TESTING: Loss: 4.972 | Acc: 51.390% (5139/10000)\n",
            "\n",
            "Epoch: 49\n",
            "Train Epoch: 49 [0/15000 (0%)]\tLoss: 0.007790, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 49 [6400/15000 (43%)]\tLoss: 0.023378, accuracy: 99.533582: 6402/6432\n",
            "Train Epoch: 49 [12800/15000 (85%)]\tLoss: 0.020234, accuracy: 99.703865: 12794/12832\n",
            "49 313 TESTING: Loss: 5.114 | Acc: 51.400% (5140/10000)\n",
            "\n",
            "Epoch: 50\n",
            "Train Epoch: 50 [0/15000 (0%)]\tLoss: 0.029136, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 50 [6400/15000 (43%)]\tLoss: 0.010048, accuracy: 99.937811: 6428/6432\n",
            "Train Epoch: 50 [12800/15000 (85%)]\tLoss: 0.009631, accuracy: 99.961035: 12827/12832\n",
            "50 313 TESTING: Loss: 5.365 | Acc: 51.540% (5154/10000)\n",
            "\n",
            "Epoch: 51\n",
            "Train Epoch: 51 [0/15000 (0%)]\tLoss: 0.003825, accuracy: 100.000000: 32/32\n",
            "Train Epoch: 51 [6400/15000 (43%)]\tLoss: 0.007134, accuracy: 99.953358: 6429/6432\n",
            "Train Epoch: 51 [12800/15000 (85%)]\tLoss: 0.017895, accuracy: 99.610349: 12782/12832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fc61d36fa66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b2fb47f66daf>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, testData)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFEN44geP24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNmodel:\n",
        "  def __init__(self,net):\n",
        "    self.net = net().to(dev)\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = optim.Adam(self.net.parameters())\n",
        "    self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[40, 70], gamma=0.5)\n",
        "  \n",
        "  def train(self, epoch, dataLoader, log_interval=200):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    self.net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    self.scheduler.step(epoch)\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataLoader):\n",
        "        data, targets = inputs.to(dev), targets.to(dev)\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.net(data)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, accuracy: {:.6f}: {}/{}'.format(epoch, batch_idx * len(data), len(dataLoader.dataset),100. * batch_idx / len(dataLoader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return \n",
        "  \n",
        "  def test(self,epoch, testLoader):\n",
        "    self.net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testLoader):\n",
        "            inputs, targets = inputs.to(dev), targets.to(dev)\n",
        "            outputs = self.net(inputs)\n",
        "            loss =self.criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "    print(epoch, len(testLoader), 'TESTING: Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2u-mR_31zET",
        "colab_type": "text"
      },
      "source": [
        "# the following is a defender model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77jugDnU1yNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "defender = NNmodel(Net)\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+75):\n",
        "    defender.train(epoch, dataLoader=testloader)\n",
        "    defender.test(epoch, testLoader=loader_target_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcV27cRoCNdY",
        "colab_type": "text"
      },
      "source": [
        "## explore confidence values between member and non-member"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nid7UOhOjznq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TargetModelInference(target_model, dataset, defender_net=None, is_train=True):\n",
        "    target_model.to(dev)\n",
        "    target_model.eval()\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=2)\n",
        "    \n",
        "    x = torch.tensor([]).to(dev)\n",
        "    y = torch.tensor([])\n",
        "    classes = torch.tensor([], dtype=torch.long)\n",
        "    \n",
        "    if defender_net is not None and is_train:\n",
        "      global defender_x \n",
        "      global defender_y\n",
        "      global defender_c\n",
        "      defender_x = torch.tensor([]).to(dev)\n",
        "      defender_y = torch.tensor([])\n",
        "      defender_c = torch.tensor([], dtype=torch.long)\n",
        "      defender_net.to(dev)\n",
        "      defender_net.eval()\n",
        "      \n",
        "    with torch.no_grad(): # generate target model's posteriors\n",
        "      for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "            inputs, targets = inputs.to(dev), targets.to('cpu')\n",
        "#             print(targets.shape)\n",
        "#             print(classes.shape)\n",
        "            \n",
        "            # get defender's posteriors for training dataset\n",
        "            if defender_net is not None and is_train:\n",
        "              defender_outputs = F.softmax(defender_net(inputs), dim=1)\n",
        "              defender_x =  torch.cat([defender_x, defender_outputs], dim=0)\n",
        "              defender_c = torch.cat([defender_c, targets], dim=0)\n",
        "              defender_y = y = torch.ones(defender_x.shape[0])\n",
        "              \n",
        "#             outputs = torch.cat([outputs, targets.float()], dim=1)\n",
        "            outputs = F.softmax(net(inputs), dim=1)\n",
        "            classes = torch.cat([classes, targets], dim=0)\n",
        "            x = torch.cat([x, outputs], dim=0)\n",
        "    if is_train:\n",
        "      y = torch.ones(x.shape[0])\n",
        "    else:\n",
        "      y = torch.zeros(x.shape[0])\n",
        "      \n",
        "    if defender_net is not None and is_train:\n",
        "      defender_x = defender_x.to('cpu')\n",
        "      defender_y = defender_y.int().to('cpu')\n",
        "      defender_c = defender_c.to('cpu')\n",
        "    return x.to('cpu'), y.int().to('cpu'), classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mjc_X45oRtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_in, y_in, c_in = TargetModelInference(net, Dataset_target_train, defender.net, is_train=True)\n",
        "x_out, y_out, c_out = TargetModelInference(net, testset, is_train=False)\n",
        "\n",
        "\n",
        "if len(testset) < len(Dataset_target_train):\n",
        "  num_samples = len(testset)\n",
        "  ind = torch.randperm(len(Dataset_target_train))[0:num_samples]\n",
        "  x_in, y_in, c_in = x_in[ind], y_in[ind], c_in[ind]\n",
        "  if defender_x is not None:\n",
        "    defender_x, defender_y, defender_c = defender_x[ind], defender_y[ind], defender_c[ind]\n",
        "else:\n",
        "  num_samples = len(Dataset_target_train)\n",
        "  ind = torch.randperm(len(testset))[0:num_samples]\n",
        "  x_out, y_out, c_out = x_out[ind], y_out[ind], c_out[ind]\n",
        "  \n",
        "  \n",
        "print('test acc for attack model input', torch.argmax(x_out, dim=1).eq(c_out).sum().item()/x_out.shape[0])\n",
        "print('ttain acc for attack model input',torch.argmax(x_in, dim=1).eq(c_in).sum().item()/x_in.shape[0])\n",
        "\n",
        "print(x_in.shape, y_in.shape, c_in.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0bEZXEI2Uw4",
        "colab_type": "text"
      },
      "source": [
        "# explore defender posteriors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQsn4LZw2UKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(defender_x.shape)\n",
        "print(defender_y.shape)\n",
        "print(defender_c.shape)\n",
        "\n",
        "print('acc for defender model input', torch.argmax(defender_x, dim=1).eq(defender_c).sum().item()/defender_x.shape[0])\n",
        "\n",
        "values, indices = torch.max(defender_x, dim=1)\n",
        "print(indices)\n",
        "print(values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hitn-kSo7Dst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_class = 10\n",
        "\n",
        "def swap_defender_posteriors(number_class, defender_x, target_x, defender_c):\n",
        "  newXs = None\n",
        "  _, class_ids = torch.max(target_x, dim=-1)\n",
        "  print(class_ids.shape)\n",
        "  print(defender_x.shape)\n",
        "  for i in range(0, defender_x.shape[0]):\n",
        "    x = defender_x[i, :]\n",
        "    class_id = class_ids[i]\n",
        "    values, ind_max = torch.max(x, dim=-1)\n",
        "    \n",
        "    indices = list(range(0, number_class))\n",
        "    indices[class_id], indices[ind_max] = indices[ind_max], indices[class_id]\n",
        "    indices = torch.LongTensor(indices)\n",
        "\n",
        "    x = x[indices].unsqueeze(dim=0)\n",
        "\n",
        "    if newXs is None: \n",
        "      newXs = x\n",
        "    else:\n",
        "      newXs = torch.cat([newXs, x], dim=0)\n",
        "      \n",
        "  print('acc of randomized defender model posteriors', torch.argmax(newXs, dim=1).eq(defender_c).sum().item()/newXs.shape[0])\n",
        "  return newXs \n",
        "\n",
        "new_defender_x = swap_defender_posteriors(number_class, defender_x, x_in, defender_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV_V9_q9lxUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balance_samples(x_in, y_in, c_in, x_out, y_out, c_out):\n",
        "  if len(testset) < len(Dataset_target_train):\n",
        "    num_samples = len(testset)\n",
        "    ind = torch.randperm(len(Dataset_target_train))[0:num_samples]\n",
        "    x_in, y_in, c_in = x_in[ind], y_in[ind], c_in[ind]\n",
        "  else:\n",
        "    num_samples = len(Dataset_target_train)\n",
        "    ind = torch.randperm(len(testset))[0:num_samples]\n",
        "    x_out, y_out, c_out = x_out[ind], y_out[ind], c_out[ind]\n",
        "\n",
        "  \n",
        "  print('train acc:',torch.argmax(x_in, dim=1).eq(c_in).sum().item()/x_in.shape[0])\n",
        "  print('test acc:', torch.argmax(x_out, dim=1).eq(c_out).sum().item()/x_out.shape[0])\n",
        "  \n",
        "  print('in data size')\n",
        "  print(x_in.shape, y_in.shape, c_in.shape)\n",
        "  print('out data size')\n",
        "  print(x_out.shape, y_out.shape, c_out.shape)\n",
        "  \n",
        "  return x_in, y_in, c_in, x_out, y_out, c_out\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4VnXMxtCbCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "in_entropy = -(x_in*x_in.log()).sum(dim=-1)\n",
        "out_entropy = -(x_out*x_out.log()).sum(dim=-1)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(in_entropy.shape)\n",
        "print(out_entropy.shape)\n",
        "\n",
        "x = in_entropy.numpy()\n",
        "y = out_entropy.numpy()\n",
        "\n",
        "print(x)\n",
        "plt.hist([x, y], label=['member', 'non-member'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qcafi6JNss5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats\n",
        "x_np =  x_in.numpy()\n",
        "in_entropy = []\n",
        "print(x_in.numpy().shape)\n",
        "for i in range(0, x_np.shape[0]):\n",
        "  in_entropy.append(scipy.stats.entropy(x_np[i][:]))\n",
        "\n",
        "x_np = x_out.numpy()\n",
        "out_entropy = []\n",
        "for i in range(0, x_np.shape[0]):\n",
        "  out_entropy.append(scipy.stats.entropy(x_np[i][:]))\n",
        "  \n",
        "plt.hist([in_entropy, out_entropy], label=['in_entropy', 'out_entropy'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5vgRjVKQvTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_sorted, indices = torch.sort(torch.tensor(in_entropy))\n",
        "out_sorted, indices = torch.sort(torch.tensor(out_entropy))\n",
        "\n",
        "\n",
        "for thres in np.arange(0, 1.4, 0.01):\n",
        "  TP = (in_sorted < thres).sum().item()\n",
        "  FP = (out_sorted < thres).sum().item()\n",
        "\n",
        "  FN = (in_sorted >= thres).sum().item()\n",
        "  TN = (out_sorted >= thres).sum().item()\n",
        "\n",
        "\n",
        "  if TP != 0:\n",
        "    p = TP / (TP + FP)\n",
        "    r = TP / (TP + FN)\n",
        "  else:\n",
        "    p = 0\n",
        "    r = 0\n",
        "\n",
        "  print('thres:', thres, 'precision: ', p, 'recall', r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iuWO37heSBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "def plot_pca(x_in, x_out, samples=1000):\n",
        "  pca = PCA(n_components=2)\n",
        "  upper_bound_1 = x_in.shape[0]\n",
        "  upper_bound_2 = x_out.shape[0]\n",
        "  \n",
        "  num_samples = min(x_in.shape[0], x_out.shape[0], samples)\n",
        "  \n",
        "  ind1 = torch.randperm(upper_bound_1)[0:num_samples]\n",
        "  ind2 = torch.randperm(upper_bound_2)[0:num_samples]\n",
        "  \n",
        "  p_in = pca.fit_transform(x_in[ind1].numpy())\n",
        "  p_out = pca.fit_transform(x_out[ind2].numpy())\n",
        "  \n",
        "  print(ind1.shape)\n",
        "  \n",
        "  plt.scatter(p_in[:,0], p_in[:,1], label='member')\n",
        "  plt.scatter(p_out[:,0], p_out[:,1], label='non-member')\n",
        "\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()\n",
        "  \n",
        "  x_in_sorted = torch.sort(x_in[ind1], dim=-1,  descending=True)\n",
        "  x_out_sorted = torch.sort(x_out[ind2], dim=-1,  descending=True)\n",
        "\n",
        "  p_in = pca.fit_transform(x_in[:, :3].numpy())\n",
        "  p_out = pca.fit_transform(x_out[:, :3].numpy())\n",
        "  print(ind1.shape)\n",
        "\n",
        "  plt.scatter(p_in[:,0], p_in[:,1], label='member')\n",
        "  plt.scatter(p_out[:,0], p_out[:,1], label='non-member')\n",
        "\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTT_0JAnNQAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_pca(x_in, x_out, samples=3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B22kkDnwD7Mk",
        "colab_type": "text"
      },
      "source": [
        "### find membership's nearest non-membership neighbour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cUnsUyIEIcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diffs1 = []\n",
        "with torch.no_grad():\n",
        "  temp_in = torch.utils.data.DataLoader(x_in, batch_size=1, shuffle=False)\n",
        "  pdist = nn.PairwiseDistance(p=2)\n",
        "  for i, x in enumerate(temp_in):\n",
        "    diff_norm = torch.norm(torch.sub(x_out, x), p=2, dim=-1)\n",
        "    diffs1.append(diff_norm.min().item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eMLK5tLIZZR",
        "colab_type": "code",
        "outputId": "06d2460f-054e-4d60-c845-b6a7a70d610d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "diffs1 = torch.tensor(diffs1).numpy()\n",
        "print((diffs1==0).sum())\n",
        "hist, bin_edges = np.histogram(diffs1, bins=100)\n",
        "print(list(zip(hist, bin_edges)))\n",
        "\n",
        "plt.hist([diffs1], label=['neareast dist'], bins=100)\n",
        "print(bin)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "[(9137, 0.0), (402, 0.0024253712), (151, 0.0048507424), (86, 0.0072761136), (52, 0.009701485), (31, 0.0121268565), (21, 0.014552227), (34, 0.016977599), (18, 0.01940297), (6, 0.021828342), (12, 0.024253713), (8, 0.026679084), (13, 0.029104454), (2, 0.031529825), (2, 0.033955198), (2, 0.03638057), (2, 0.03880594), (3, 0.04123131), (0, 0.043656684), (1, 0.046082053), (1, 0.048507426), (2, 0.050932795), (1, 0.053358167), (3, 0.05578354), (1, 0.05820891), (1, 0.06063428), (1, 0.06305965), (1, 0.06548502), (1, 0.067910396), (0, 0.07033577), (1, 0.07276114), (0, 0.075186506), (0, 0.07761188), (1, 0.08003725), (0, 0.08246262), (0, 0.084888), (1, 0.08731337), (0, 0.089738734), (0, 0.09216411), (0, 0.09458948), (1, 0.09701485), (0, 0.099440224), (0, 0.10186559), (0, 0.10429096), (0, 0.106716335), (0, 0.10914171), (0, 0.11156708), (0, 0.11399245), (0, 0.11641782), (0, 0.11884319), (0, 0.12126856), (0, 0.123693936), (0, 0.1261193), (0, 0.12854467), (0, 0.13097005), (0, 0.13339542), (0, 0.13582079), (0, 0.13824616), (0, 0.14067154), (0, 0.14309691), (0, 0.14552228), (0, 0.14794765), (0, 0.15037301), (0, 0.15279838), (0, 0.15522376), (0, 0.15764913), (0, 0.1600745), (0, 0.16249987), (0, 0.16492525), (0, 0.16735062), (0, 0.169776), (0, 0.17220137), (0, 0.17462674), (0, 0.1770521), (0, 0.17947747), (0, 0.18190284), (0, 0.18432821), (0, 0.18675359), (0, 0.18917896), (0, 0.19160433), (0, 0.1940297), (0, 0.19645508), (0, 0.19888045), (0, 0.20130582), (0, 0.20373118), (0, 0.20615655), (0, 0.20858192), (0, 0.2110073), (0, 0.21343267), (0, 0.21585804), (0, 0.21828341), (0, 0.22070879), (0, 0.22313416), (0, 0.22555953), (0, 0.2279849), (0, 0.23041026), (0, 0.23283564), (0, 0.23526101), (0, 0.23768638), (1, 0.24011175)]\n",
            "<built-in function bin>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSE7a5uIJ9rx",
        "colab_type": "text"
      },
      "source": [
        "### find non-membership's nearest membership neighbour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLdWsc4UJx-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diffs2 = []\n",
        "with torch.no_grad():\n",
        "  temp_out = torch.utils.data.DataLoader(x_out, batch_size=1, shuffle=False)\n",
        "  \n",
        "  for i, x in enumerate(temp_out):\n",
        "    diff_norm = torch.norm(torch.sub(x_in, x), p=2, dim=1)\n",
        "    diffs2.append(diff_norm.min().item())\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3XK_6LeKNuf",
        "colab_type": "code",
        "outputId": "15211579-f7fe-467a-bc06-43bc0511df50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "diffs2 = torch.tensor(diffs2)\n",
        "print((diffs2<10e-6).sum())\n",
        "hist, bin_edges = np.histogram(diffs2)\n",
        "\n",
        "print(list(zip(hist, bin_edges)))\n",
        "\n",
        "diffs2 = torch.tensor(diffs2).numpy()\n",
        "# plt.xlim([0, 0.005])\n",
        "plt.hist([diffs2], label=['neareast dist'], bins=20)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3492)\n",
            "[(7985, 0.0), (535, 0.06774731), (318, 0.13549462), (271, 0.20324194), (242, 0.27098924), (236, 0.33873656), (219, 0.4064839), (124, 0.47423118), (58, 0.5419785), (12, 0.60972583)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3k8VMF_crWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "def cdf(data):\n",
        "\n",
        "    data_size=len(data)\n",
        "\n",
        "    # Set bins edges\n",
        "    data_set=sorted(set(data))\n",
        "    bins=np.append(data_set, data_set[-1]+1)\n",
        "\n",
        "    # Use the histogram function to bin the data\n",
        "    counts, bin_edges = np.histogram(data, bins=bins, density=False)\n",
        "\n",
        "    counts=counts.astype(float)/data_size\n",
        "\n",
        "    # Find the cdf\n",
        "    cdf = np.cumsum(counts)\n",
        "\n",
        "    x = bin_edges[0:-1]\n",
        "    y = cdf\n",
        "\n",
        "    f = interp1d(x, y)\n",
        "    f2 = interp1d(x, y, kind='cubic')\n",
        "\n",
        "    xnew = np.linspace(0, max(x), num=1000, endpoint=True)\n",
        "\n",
        "    # Plot the cdf\n",
        "    plt.plot(x, y)\n",
        "    plt.legend(['data'], loc='best')\n",
        "    plt.title(\"cdf\")\n",
        "    plt.ylim((0,1))\n",
        "    plt.ylabel(\"CDF\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAiLVTo1f8zc",
        "colab_type": "code",
        "outputId": "1d9f7a91-373b-4427-8c07-0ea69ba54ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "    # Plot the cdf\n",
        "thres = 10e-17\n",
        "num1 = []\n",
        "num2 = []\n",
        "\n",
        "plot_x = []\n",
        "\n",
        "for i in range(0, 17):\n",
        "  num1.append((diffs1<thres).sum()/len(diffs1))\n",
        "  num2.append((diffs2<thres).sum()/len(diffs2))\n",
        "  plot_x.append(thres)\n",
        "  \n",
        "  thres = thres * 10\n",
        "plt.xscale('log')\n",
        "plt.plot(plot_x, num1, '-o', plot_x, num2, '-*')\n",
        "plt.legend(['members', 'nom-member'], loc='best')\n",
        "plt.title(\"number of points less than threshold\")\n",
        "plt.xlabel(\"threshold\")\n",
        "plt.ylabel(\"number per\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEoX4Zqjesqy",
        "colab_type": "text"
      },
      "source": [
        "## prepare attacking data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mUgV830YpaU1",
        "outputId": "e2712e1f-656b-481c-8ab3-05a1dd58529e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_datasets = {}\n",
        "test_datasets = {}\n",
        "class_num = 10\n",
        "def get_ds_per_class(x_in,y_in, c_in, x_out, y_out, c_out, train_datasets, test_datasets, class_num = 10):\n",
        "  train_dls = {}\n",
        "  test_dls = {}\n",
        "  global  test_accs# a global var for plot\n",
        "  test_accs = []\n",
        "  \n",
        "  for i in range(0, class_num):\n",
        "    print(x_in.shape, y_in.shape, c_in.shape)\n",
        "    ind_in = (c_in==i).nonzero().squeeze()\n",
        "    ind_out = (c_out==i).nonzero().squeeze()\n",
        "\n",
        "    x_in_i, y_in_i = x_in[ind_in], y_in[ind_in]\n",
        "    x_out_i, y_out_i = x_out[ind_out], y_out[ind_out]\n",
        "\n",
        "    num_in = x_in_i.shape[0]\n",
        "    num_out = x_out_i.shape[0]\n",
        "    num_in_train = int(x_in_i.shape[0]*ration_attack_train)\n",
        "    num_out_train = int(x_out_i.shape[0]*ration_attack_train)\n",
        "\n",
        "    train_datasets[i] = TensorDataset(torch.cat([x_in_i[0:num_in_train], x_out_i[0:num_out_train]], dim=0), torch.cat([y_in_i[0:num_in_train], y_out_i[0:num_out_train]], dim=0))\n",
        "    test_datasets[i] = TensorDataset(torch.cat([x_in_i[num_in_train:], x_out_i[num_out_train:]], dim=0), torch.cat([y_in_i[num_in_train:], y_out_i[num_out_train:]], dim=0))\n",
        "\n",
        "    train_dls[i] = torch.utils.data.DataLoader(train_datasets[i], batch_size=bz, shuffle=True)\n",
        "    test_dls[i] = torch.utils.data.DataLoader(test_datasets[i], batch_size=bz, shuffle=False)\n",
        "\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    for x_, y_ in train_dls[i]:\n",
        "      if x_.shape[0] == bz:\n",
        "        ind_train = (y_==1).nonzero().squeeze(dim=1)\n",
        "        train_correct +=  torch.argmax(x_[ind_train], dim=1).eq(i).sum().item()\n",
        "        train_total += x_[ind_train].shape[0]\n",
        "\n",
        "        ind_test = (y_==0).nonzero().squeeze(dim=1)\n",
        "        test_correct += torch.argmax(x_[ind_test], dim=1).eq(i).sum().item()\n",
        "        test_total += x_[ind_test].shape[0]\n",
        "    test_accs.append(test_correct/test_total)\n",
        "    print('class :{}, in_dataset of target model #correct/total={}/{} = {}'.format(i, train_correct, train_total, train_correct/train_total))\n",
        "    print('class :{}, out_dataset of target model #correct/total={}/{} = {}'.format(i, test_correct, test_total, test_correct/test_total))\n",
        "    \n",
        "get_ds_per_class(x_in,y_in, c_in, x_out, y_out, c_out, train_datasets, test_datasets)\n",
        "\n",
        "train_dls = {}\n",
        "test_dls = {}\n",
        "  \n",
        "for i in range(0, class_num):\n",
        "    train_dls[i] = torch.utils.data.DataLoader(train_datasets[i], batch_size=bz, shuffle=True)\n",
        "    test_dls[i] = torch.utils.data.DataLoader(test_datasets[i], batch_size=bz, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :0, in_dataset of target model #correct/total=690/690 = 1.0\n",
            "class :0, out_dataset of target model #correct/total=393/686 = 0.5728862973760933\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :1, in_dataset of target model #correct/total=711/711 = 1.0\n",
            "class :1, out_dataset of target model #correct/total=440/697 = 0.6312769010043041\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :2, in_dataset of target model #correct/total=712/712 = 1.0\n",
            "class :2, out_dataset of target model #correct/total=292/696 = 0.41954022988505746\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :3, in_dataset of target model #correct/total=686/686 = 1.0\n",
            "class :3, out_dataset of target model #correct/total=259/690 = 0.3753623188405797\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :4, in_dataset of target model #correct/total=684/684 = 1.0\n",
            "class :4, out_dataset of target model #correct/total=311/692 = 0.44942196531791906\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :5, in_dataset of target model #correct/total=696/696 = 1.0\n",
            "class :5, out_dataset of target model #correct/total=303/680 = 0.4455882352941177\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :6, in_dataset of target model #correct/total=691/691 = 1.0\n",
            "class :6, out_dataset of target model #correct/total=421/685 = 0.6145985401459854\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :7, in_dataset of target model #correct/total=679/679 = 1.0\n",
            "class :7, out_dataset of target model #correct/total=390/697 = 0.5595408895265424\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :8, in_dataset of target model #correct/total=687/687 = 1.0\n",
            "class :8, out_dataset of target model #correct/total=453/689 = 0.6574746008708273\n",
            "torch.Size([10000, 10]) torch.Size([10000]) torch.Size([10000])\n",
            "class :9, in_dataset of target model #correct/total=678/678 = 1.0\n",
            "class :9, out_dataset of target model #correct/total=415/698 = 0.5945558739255015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TJfvD-FiMJF",
        "colab_type": "text"
      },
      "source": [
        "## pca per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlkJb9mjfa1r",
        "colab_type": "code",
        "outputId": "1776e7cc-c4f5-411d-b242-d8852d41aba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "for i in range(0, class_num):\n",
        "  x_i, y_i = train_datasets[i][:]\n",
        "  member = (y_i.squeeze()==1).nonzero().squeeze()\n",
        "  nonmember = (y_i.squeeze()==0).nonzero().squeeze()\n",
        "  \n",
        "  print('class: {}\\t member num:{}\\t non member num:{}'.format(i, member.shape[0], nonmember.shape[0]))\n",
        "  plot_pca(x_i[member], x_i[nonmember], samples=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class: 0\t member num:700\t non member num:700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9640e69e5d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class: {}\\t member num:{}\\t non member num:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonmember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mplot_pca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnonmember\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_pca' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEH3xNbjJoZk",
        "colab_type": "text"
      },
      "source": [
        "### attack model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YjAIVVfJtqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_score(target, pred):\n",
        "  \n",
        "  target = target.squeeze()\n",
        "  pred = pred.squeeze()\n",
        "  \n",
        "  \n",
        "  TP = ((pred == 1) & (target == 1)).sum().item()\n",
        "  \n",
        "  TN = ((pred == 0) & (target == 0)).sum().item()\n",
        "  \n",
        "  FN = ((pred == 0) & (target == 1)).sum().item()\n",
        "\n",
        "  FP = ((pred == 1) & (target == 0)).sum().item()\n",
        "  \n",
        "  if TP != 0:\n",
        "    p = TP / (TP + FP)\n",
        "    r = TP / (TP + FN)\n",
        "  else:\n",
        "    p = 0\n",
        "    r = 0\n",
        "  \n",
        "  return p,r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE51mqotmxW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_scores(target, pred):\n",
        "  target = target.squeeze()\n",
        "  pred = pred.squeeze()\n",
        "  \n",
        "  \n",
        "  TP = ((pred == 1) & (target == 1)).sum().item()\n",
        "  \n",
        "  TN = ((pred == 0) & (target == 0)).sum().item()\n",
        "  \n",
        "  FN = ((pred == 0) & (target == 1)).sum().item()\n",
        "\n",
        "  FP = ((pred == 1) & (target == 0)).sum().item()\n",
        "  \n",
        "  \n",
        "  return TP, TN, FN, FP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2p2DirSJWUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class attackModel(nn.Module):\n",
        "    def __init__(self, num_in_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_in_features, 128)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(64,64)\n",
        "        self.fc4 = nn.Linear(64,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.relu(self.fc2(x)))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DJXuHUvJuVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_attack_model(dataloader, model, optimizer, criterion):\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  y_total = torch.tensor([])\n",
        "  pred_total = torch.tensor([])\n",
        "  \n",
        "  model.train()\n",
        "  for i, (x, y) in enumerate(dataloader):\n",
        "    y = y.float().unsqueeze(dim=1)\n",
        "    \n",
        "    #clear cumulated gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    y_ = model(x)\n",
        "    loss = criterion(y_, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      total_loss += loss.item()\n",
        "      pred = (F.sigmoid(y_) > 0.5).float()\n",
        "      y_total = torch.cat([y_total, y], dim=0)\n",
        "      pred_total = torch.cat([pred_total, pred], dim=0)\n",
        "\n",
        "      \n",
        "  acc = pred_total.eq(y_total).sum().item()/pred_total.shape[0]\n",
        "  print('loss: {}\\tacc: {}'.format(loss.item()/(i+1), acc))\n",
        "  print(binary_score(y_total, pred_total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMseMy0Zsbla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def test_attack_model(testloader, net, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    \n",
        "    y_total = torch.tensor([])\n",
        "    pred_total = torch.tensor([])\n",
        "    times = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for i, (x, y) in enumerate(testloader):\n",
        "            y = y.float().unsqueeze(dim=1)\n",
        "          \n",
        "            y_ = net(x)\n",
        "            loss = criterion(y_, y)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            \n",
        "            pred = (F.sigmoid(y_) > 0.5).float()\n",
        "            y_total = torch.cat([y_total, y], dim=0)\n",
        "            pred_total = torch.cat([pred_total, pred], dim=0)\n",
        "            times = i\n",
        "            \n",
        "    acc = pred_total.eq(y_total).sum().item()/pred_total.shape[0]\n",
        "#     attack_acc.append(acc)\n",
        "    p, r = binary_score(y_total, pred_total)\n",
        "\n",
        "    print('TESTING: Loss: {} | Acc: {} \\t'.format(test_loss/times, acc))\n",
        "    print(binary_score(y_total, pred_total))\n",
        "    return test_scores(y_total, pred_total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwyzvePnUFCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttackModelBundle:\n",
        "  '''only one model graph exists. \n",
        "     Switching between different models is achieved by loading and saveing different params\n",
        "     indexed by class_id.\n",
        "  '''\n",
        "  def __init__(self, num_classes, Model):\n",
        "    self.num_classes = num_classes\n",
        "    self.ModelClass = Model\n",
        "    self.attack_model = Model(num_classes)\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "  \n",
        "  def __get_path__(self, class_id):\n",
        "    return './attackModelClass{}.pt'.format(class_id)\n",
        "    \n",
        "  def __reset_model__(self):\n",
        "    del self.attack_model\n",
        "    self.attack_model = self.ModelClass(self.num_classes)\n",
        "    \n",
        "  def train_class_model(self,class_id, train_dataloader, test_dataloader=None, epoch=3,is_new=True):\n",
        "    print('*'*10, '\\tTraing new model on class: {} \\t'.format(class_id), '*'*10)\n",
        "    if is_new:\n",
        "      self.__reset_model__()\n",
        "    optimizer = optim.Adam(self.attack_model.parameters(), lr=0.001, weight_decay=10**-7)\n",
        "    \n",
        "    for i in range(0, epoch):\n",
        "      train_attack_model(train_dataloader, self.attack_model, optimizer, self.criterion)\n",
        "      if test_dataloader is not None:\n",
        "        test_attack_model(test_dataloader, self.attack_model, self.criterion)\n",
        "    \n",
        "    path = './attackModelClass{}.pt'.format(class_id)\n",
        "    torch.save(self.attack_model.state_dict(), path)\n",
        "    \n",
        "    \n",
        "  def test_class_model(self, class_id, dataloader):\n",
        "    path = self. __get_path__(class_id)\n",
        "    self.attack_model.load_state_dict(torch.load(path))\n",
        "    print(dataloader)\n",
        "    return test_attack_model(dataloader, self.attack_model, self.criterion)\n",
        "    \n",
        "  def test_all_classes(self, class_num, dataloader_dic):\n",
        "    global attack_precision\n",
        "    global attack_recall\n",
        "    attack_precision = []\n",
        "    attack_recall = []\n",
        "    \n",
        "    TP_total, TN_total, FN_total, FP_total = 0, 0, 0, 0\n",
        "\n",
        "    for i in range(class_num):\n",
        "      print('debug key error:', i)\n",
        "      dl = dataloader_dic[i]\n",
        "      TP, TN, FN, FP = self.test_class_model(i, dl)\n",
        "      TP_total += TP\n",
        "      TN_total += TN\n",
        "      FN_total += FN\n",
        "      FP_total += FP\n",
        "      \n",
        "      attack_precision.append(TP/(TP+FP))\n",
        "      attack_recall.append(TP/(TP+FN))\n",
        "    p = TP_total / (TP_total + FP_total)\n",
        "    r = TP_total / (TP_total + FN_total)\n",
        "    \n",
        "\n",
        "    print('TESTING precision: {}, recall: {}'.format(p, r))\n",
        "    \n",
        "    print('confusion matrix:')\n",
        "    print(f'TN: {TN_total}\\tFP: {FP_total}\\nFN: {FN_total}\\t TP:{TP_total}')\n",
        "    return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Ce9oPRlNtc",
        "colab_type": "code",
        "outputId": "2caf5c92-540e-4b46-9bd6-7be806a5811b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mab = AttackModelBundle(10, Model=attackModel)\n",
        "\n",
        "for i in range(10):\n",
        "  mab.train_class_model(i, train_dls[i], test_dls[i], epoch=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********** \tTraing new model on class: 0 \t **********\n",
            "loss: 0.011119073087518866\tacc: 0.6357142857142857\n",
            "(0.5786423841059603, 0.9985714285714286)\n",
            "TESTING: Loss: 0.6017082350121604 | Acc: 0.7183333333333334 \t\n",
            "(0.6396588486140725, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.01055411926724694\tacc: 0.7357142857142858\n",
            "(0.6574427480916031, 0.9842857142857143)\n",
            "TESTING: Loss: 0.5393604089816412 | Acc: 0.7266666666666667 \t\n",
            "(0.6471861471861472, 0.9966666666666667)\n",
            "loss: 0.011333441192453558\tacc: 0.7414285714285714\n",
            "(0.670020120724346, 0.9514285714285714)\n",
            "TESTING: Loss: 0.5387270831399493 | Acc: 0.7266666666666667 \t\n",
            "(0.6478260869565218, 0.9933333333333333)\n",
            "loss: 0.01101046326485547\tacc: 0.7428571428571429\n",
            "(0.6640926640926641, 0.9828571428571429)\n",
            "TESTING: Loss: 0.5415935549471114 | Acc: 0.725 \t\n",
            "(0.6457883369330454, 0.9966666666666667)\n",
            "loss: 0.011014683680100874\tacc: 0.7528571428571429\n",
            "(0.6715116279069767, 0.99)\n",
            "TESTING: Loss: 0.5330895516607497 | Acc: 0.7266666666666667 \t\n",
            "(0.6478260869565218, 0.9933333333333333)\n",
            "loss: 0.009563412855971943\tacc: 0.7492857142857143\n",
            "(0.6685990338164252, 0.9885714285714285)\n",
            "TESTING: Loss: 0.5337464809417725 | Acc: 0.7266666666666667 \t\n",
            "(0.6478260869565218, 0.9933333333333333)\n",
            "loss: 0.010405004701831123\tacc: 0.7492857142857143\n",
            "(0.6709108716944172, 0.9785714285714285)\n",
            "TESTING: Loss: 0.5277876042657428 | Acc: 0.7316666666666667 \t\n",
            "(0.6520787746170679, 0.9933333333333333)\n",
            "loss: 0.009033751081336628\tacc: 0.7507142857142857\n",
            "(0.6729064039408867, 0.9757142857142858)\n",
            "TESTING: Loss: 0.5259487066004012 | Acc: 0.735 \t\n",
            "(0.656319290465632, 0.9866666666666667)\n",
            "loss: 0.014103655110705982\tacc: 0.7542857142857143\n",
            "(0.6724806201550387, 0.9914285714285714)\n",
            "TESTING: Loss: 0.5294147829214731 | Acc: 0.7316666666666667 \t\n",
            "(0.6520787746170679, 0.9933333333333333)\n",
            "loss: 0.0086701593615792\tacc: 0.7557142857142857\n",
            "(0.6748046875, 0.9871428571428571)\n",
            "TESTING: Loss: 0.5267709460523393 | Acc: 0.735 \t\n",
            "(0.6556291390728477, 0.99)\n",
            "********** \tTraing new model on class: 1 \t **********\n",
            "loss: 0.01205581823984782\tacc: 0.6320621468926554\n",
            "(0.630173564753004, 0.659217877094972)\n",
            "TESTING: Loss: 0.6209466871288087 | Acc: 0.7018121911037891 \t\n",
            "(0.6290983606557377, 1.0)\n",
            "loss: 0.009716908799277411\tacc: 0.6913841807909604\n",
            "(0.6302521008403361, 0.9427374301675978)\n",
            "TESTING: Loss: 0.5510145823160807 | Acc: 0.7182866556836903 \t\n",
            "(0.6422594142259415, 1.0)\n",
            "loss: 0.01007018354203966\tacc: 0.7062146892655368\n",
            "(0.6415094339622641, 0.9497206703910615)\n",
            "TESTING: Loss: 0.5401830209626092 | Acc: 0.7248764415156508 \t\n",
            "(0.6483050847457628, 0.996742671009772)\n",
            "loss: 0.013592400815751818\tacc: 0.7168079096045198\n",
            "(0.642792384406165, 0.9902234636871509)\n",
            "TESTING: Loss: 0.537430077791214 | Acc: 0.7248764415156508 \t\n",
            "(0.6483050847457628, 0.996742671009772)\n",
            "loss: 0.01342884169684516\tacc: 0.7161016949152542\n",
            "(0.6440366972477064, 0.9804469273743017)\n",
            "TESTING: Loss: 0.5337454196479585 | Acc: 0.7314662273476112 \t\n",
            "(0.6538461538461539, 0.996742671009772)\n",
            "loss: 0.011825847625732421\tacc: 0.7168079096045198\n",
            "(0.6456984273820536, 0.9748603351955307)\n",
            "TESTING: Loss: 0.532628176940812 | Acc: 0.7364085667215815 \t\n",
            "(0.6580645161290323, 0.996742671009772)\n",
            "loss: 0.014124250411987305\tacc: 0.721045197740113\n",
            "(0.6457765667574932, 0.9930167597765364)\n",
            "TESTING: Loss: 0.5323912236425612 | Acc: 0.7413509060955519 \t\n",
            "(0.6623376623376623, 0.996742671009772)\n",
            "loss: 0.01088037159707811\tacc: 0.7139830508474576\n",
            "(0.6420091324200913, 0.9818435754189944)\n",
            "TESTING: Loss: 0.5374900433752272 | Acc: 0.7265238879736409 \t\n",
            "(0.6496815286624203, 0.996742671009772)\n",
            "loss: 0.01065401832262675\tacc: 0.7189265536723164\n",
            "(0.64376130198915, 0.994413407821229)\n",
            "TESTING: Loss: 0.5336825235022439 | Acc: 0.7347611202635914 \t\n",
            "(0.6566523605150214, 0.996742671009772)\n",
            "loss: 0.0115430129898919\tacc: 0.7175141242937854\n",
            "(0.6438979963570127, 0.9874301675977654)\n",
            "TESTING: Loss: 0.5374070554971695 | Acc: 0.7265238879736409 \t\n",
            "(0.6496815286624203, 0.996742671009772)\n",
            "********** \tTraing new model on class: 2 \t **********\n",
            "loss: 0.01001954608493381\tacc: 0.7378091872791519\n",
            "(0.6586715867158671, 0.9986013986013986)\n",
            "TESTING: Loss: 0.4517168203989665 | Acc: 0.8336079077429983 \t\n",
            "(0.7524509803921569, 1.0)\n",
            "loss: 0.01127375496758355\tacc: 0.8162544169611308\n",
            "(0.7377220480668757, 0.9874125874125874)\n",
            "TESTING: Loss: 0.38517723563644624 | Acc: 0.841845140032949 \t\n",
            "(0.7617866004962779, 1.0)\n",
            "loss: 0.011830230553944906\tacc: 0.8240282685512368\n",
            "(0.7457805907172996, 0.9888111888111888)\n",
            "TESTING: Loss: 0.3824491285615497 | Acc: 0.8451400329489291 \t\n",
            "(0.7655860349127181, 1.0)\n",
            "loss: 0.018730594052208796\tacc: 0.8275618374558303\n",
            "(0.7481559536354057, 0.993006993006993)\n",
            "TESTING: Loss: 0.3781554483705097 | Acc: 0.8500823723228995 \t\n",
            "(0.7727272727272727, 0.996742671009772)\n",
            "loss: 0.005815032455656264\tacc: 0.823321554770318\n",
            "(0.7444794952681388, 0.9902097902097902)\n",
            "TESTING: Loss: 0.3787065852019522 | Acc: 0.8484349258649094 \t\n",
            "(0.7707808564231738, 0.996742671009772)\n",
            "loss: 0.009525838825437758\tacc: 0.8240282685512368\n",
            "(0.7442348008385744, 0.993006993006993)\n",
            "TESTING: Loss: 0.3779891100194719 | Acc: 0.8500823723228995 \t\n",
            "(0.7727272727272727, 0.996742671009772)\n",
            "loss: 0.00802269909116957\tacc: 0.8247349823321555\n",
            "(0.7470899470899471, 0.9874125874125874)\n",
            "TESTING: Loss: 0.3758476558658812 | Acc: 0.8517298187808896 \t\n",
            "(0.7746835443037975, 0.996742671009772)\n",
            "loss: 0.004746484425332811\tacc: 0.8254416961130742\n",
            "(0.7463157894736843, 0.9916083916083916)\n",
            "TESTING: Loss: 0.3734701681468222 | Acc: 0.8500823723228995 \t\n",
            "(0.7741116751269036, 0.993485342019544)\n",
            "loss: 0.008177504936854044\tacc: 0.8254416961130742\n",
            "(0.7478813559322034, 0.9874125874125874)\n",
            "TESTING: Loss: 0.3767448945177926 | Acc: 0.8500823723228995 \t\n",
            "(0.7713567839195979, 1.0)\n",
            "loss: 0.008362137609057956\tacc: 0.8254416961130742\n",
            "(0.7473572938689218, 0.9888111888111888)\n",
            "TESTING: Loss: 0.3775183591577742 | Acc: 0.8484349258649094 \t\n",
            "(0.7694235588972431, 1.0)\n",
            "********** \tTraing new model on class: 3 \t **********\n",
            "loss: 0.009763411500237205\tacc: 0.7408470926058865\n",
            "(0.6580952380952381, 0.9971139971139971)\n",
            "TESTING: Loss: 0.466243596540557 | Acc: 0.8358458961474037 \t\n",
            "(0.7518987341772152, 1.0)\n",
            "loss: 0.0054240040481090546\tacc: 0.8399138549892319\n",
            "(0.7628635346756152, 0.9841269841269841)\n",
            "TESTING: Loss: 0.3753475348154704 | Acc: 0.8525963149078727 \t\n",
            "(0.7728459530026109, 0.9966329966329966)\n",
            "loss: 0.007328180426901037\tacc: 0.8485283560660445\n",
            "(0.7751141552511416, 0.9797979797979798)\n",
            "TESTING: Loss: 0.38313331454992294 | Acc: 0.8492462311557789 \t\n",
            "(0.7688311688311689, 0.9966329966329966)\n",
            "loss: 0.004796986891464753\tacc: 0.8528356066044508\n",
            "(0.7779043280182233, 0.9855699855699855)\n",
            "TESTING: Loss: 0.36045299884345794 | Acc: 0.8576214405360134 \t\n",
            "(0.7819148936170213, 0.98989898989899)\n",
            "loss: 0.008209238675507631\tacc: 0.8578607322325915\n",
            "(0.7848101265822784, 0.9841269841269841)\n",
            "TESTING: Loss: 0.3599696672625012 | Acc: 0.8576214405360134 \t\n",
            "(0.7819148936170213, 0.98989898989899)\n",
            "loss: 0.007753901183605194\tacc: 0.8564249820531228\n",
            "(0.7810718358038768, 0.9884559884559885)\n",
            "TESTING: Loss: 0.358878917992115 | Acc: 0.8609715242881072 \t\n",
            "(0.7907608695652174, 0.9797979797979798)\n",
            "loss: 0.004112949764186686\tacc: 0.8621679827709978\n",
            "(0.7876004592422503, 0.98989898989899)\n",
            "TESTING: Loss: 0.36025746580627227 | Acc: 0.8609715242881072 \t\n",
            "(0.7907608695652174, 0.9797979797979798)\n",
            "loss: 0.005787414583292874\tacc: 0.8621679827709978\n",
            "(0.7869415807560137, 0.9913419913419913)\n",
            "TESTING: Loss: 0.35834415670898223 | Acc: 0.8626465661641541 \t\n",
            "(0.7913279132791328, 0.9831649831649831)\n",
            "loss: 0.004429178820414977\tacc: 0.8607322325915291\n",
            "(0.7884393063583816, 0.9841269841269841)\n",
            "TESTING: Loss: 0.35912175642119515 | Acc: 0.8592964824120602 \t\n",
            "(0.7855227882037533, 0.9865319865319865)\n",
            "loss: 0.011672219092195684\tacc: 0.8628858578607322\n",
            "(0.791860465116279, 0.9826839826839827)\n",
            "TESTING: Loss: 0.36069291333357495 | Acc: 0.8592964824120602 \t\n",
            "(0.7855227882037533, 0.9865319865319865)\n",
            "********** \tTraing new model on class: 4 \t **********\n",
            "loss: 0.013895656574856152\tacc: 0.6834170854271356\n",
            "(0.6111111111111112, 1.0)\n",
            "TESTING: Loss: 0.5837307837274339 | Acc: 0.774247491638796 \t\n",
            "(0.6882217090069284, 1.0)\n",
            "loss: 0.008854590356349945\tacc: 0.7889447236180904\n",
            "(0.7102212855637513, 0.9725829725829725)\n",
            "TESTING: Loss: 0.4346924391057756 | Acc: 0.81438127090301 \t\n",
            "(0.7297297297297297, 0.9966442953020134)\n",
            "loss: 0.009570578282529657\tacc: 0.8140703517587939\n",
            "(0.735357917570499, 0.9783549783549783)\n",
            "TESTING: Loss: 0.42760249144501156 | Acc: 0.81438127090301 \t\n",
            "(0.7308641975308642, 0.9932885906040269)\n",
            "loss: 0.011287380348552357\tacc: 0.8018664752333095\n",
            "(0.7268770402611534, 0.963924963924964)\n",
            "TESTING: Loss: 0.4257956991593043 | Acc: 0.81438127090301 \t\n",
            "(0.7308641975308642, 0.9932885906040269)\n",
            "loss: 0.010688885369084099\tacc: 0.8140703517587939\n",
            "(0.7348484848484849, 0.9797979797979798)\n",
            "TESTING: Loss: 0.43011915187040967 | Acc: 0.8160535117056856 \t\n",
            "(0.7315270935960592, 0.9966442953020134)\n",
            "loss: 0.009350687265396118\tacc: 0.8183776022972002\n",
            "(0.7365591397849462, 0.9884559884559885)\n",
            "TESTING: Loss: 0.42109185126092696 | Acc: 0.822742474916388 \t\n",
            "(0.7448979591836735, 0.9798657718120806)\n",
            "loss: 0.018046113577756016\tacc: 0.8183776022972002\n",
            "(0.7370689655172413, 0.987012987012987)\n",
            "TESTING: Loss: 0.42074266572793323 | Acc: 0.822742474916388 \t\n",
            "(0.7424242424242424, 0.9865771812080537)\n",
            "loss: 0.014603437347845598\tacc: 0.8176597272074659\n",
            "(0.736275565123789, 0.987012987012987)\n",
            "TESTING: Loss: 0.42057543827427757 | Acc: 0.8210702341137124 \t\n",
            "(0.7442455242966752, 0.9765100671140939)\n",
            "loss: 0.010101333260536194\tacc: 0.8183776022972002\n",
            "(0.7391304347826086, 0.9812409812409812)\n",
            "TESTING: Loss: 0.42254969312085044 | Acc: 0.8193979933110368 \t\n",
            "(0.7461139896373057, 0.9664429530201343)\n",
            "loss: 0.009475663981654427\tacc: 0.8162239770279971\n",
            "(0.7357065803667745, 0.9841269841269841)\n",
            "TESTING: Loss: 0.42068976660569507 | Acc: 0.8193979933110368 \t\n",
            "(0.7375, 0.9899328859060402)\n",
            "********** \tTraing new model on class: 5 \t **********\n",
            "loss: 0.010678270323710009\tacc: 0.7356076759061834\n",
            "(0.699642431466031, 0.8302687411598303)\n",
            "TESTING: Loss: 0.5066028022103839 | Acc: 0.8013245033112583 \t\n",
            "(0.7169811320754716, 1.0)\n",
            "loss: 0.006618718531998721\tacc: 0.8031272210376688\n",
            "(0.7239583333333334, 0.983026874115983)\n",
            "TESTING: Loss: 0.4311313529809316 | Acc: 0.8145695364238411 \t\n",
            "(0.7307692307692307, 1.0)\n",
            "loss: 0.008265980265357277\tacc: 0.8073916133617626\n",
            "(0.7289915966386554, 0.9816124469589816)\n",
            "TESTING: Loss: 0.430442380408446 | Acc: 0.8145695364238411 \t\n",
            "(0.7307692307692307, 1.0)\n",
            "loss: 0.008384680206125433\tacc: 0.8116560056858564\n",
            "(0.7306889352818372, 0.9900990099009901)\n",
            "TESTING: Loss: 0.4222756110959583 | Acc: 0.8178807947019867 \t\n",
            "(0.7354368932038835, 0.9967105263157895)\n",
            "loss: 0.011036903343417427\tacc: 0.8088130774697939\n",
            "(0.7286012526096033, 0.9872701555869873)\n",
            "TESTING: Loss: 0.42729566991329193 | Acc: 0.8162251655629139 \t\n",
            "(0.7336561743341404, 0.9967105263157895)\n",
            "loss: 0.011089344593611631\tacc: 0.8116560056858564\n",
            "(0.7306889352818372, 0.9900990099009901)\n",
            "TESTING: Loss: 0.42133285270796883 | Acc: 0.8178807947019867 \t\n",
            "(0.7354368932038835, 0.9967105263157895)\n",
            "loss: 0.011077820577404716\tacc: 0.8137882018479033\n",
            "(0.7334732423924449, 0.9886845827439887)\n",
            "TESTING: Loss: 0.42465336951944566 | Acc: 0.8162251655629139 \t\n",
            "(0.7325301204819277, 1.0)\n",
            "loss: 0.00967168469320644\tacc: 0.814498933901919\n",
            "(0.7352320675105485, 0.9858557284299858)\n",
            "TESTING: Loss: 0.4196326980988185 | Acc: 0.8178807947019867 \t\n",
            "(0.7354368932038835, 0.9967105263157895)\n",
            "loss: 0.009705033491958271\tacc: 0.8137882018479033\n",
            "(0.7320125130344108, 0.9929278642149929)\n",
            "TESTING: Loss: 0.42166034711731804 | Acc: 0.8162251655629139 \t\n",
            "(0.7336561743341404, 0.9967105263157895)\n",
            "loss: 0.010882863944227045\tacc: 0.8152096659559346\n",
            "(0.7350157728706624, 0.9886845827439887)\n",
            "TESTING: Loss: 0.4210546496841643 | Acc: 0.8162251655629139 \t\n",
            "(0.7336561743341404, 0.9967105263157895)\n",
            "********** \tTraing new model on class: 6 \t **********\n",
            "loss: 0.014657227830453352\tacc: 0.6379187455452602\n",
            "(0.6341127922971114, 0.6557610241820768)\n",
            "TESTING: Loss: 0.644659548997879 | Acc: 0.6910299003322259 \t\n",
            "(0.6188524590163934, 1.0)\n",
            "loss: 0.012996016578240828\tacc: 0.7077690662865289\n",
            "(0.6365330848089469, 0.9715504978662873)\n",
            "TESTING: Loss: 0.5548165887594223 | Acc: 0.717607973421927 \t\n",
            "(0.6410256410256411, 0.9933774834437086)\n",
            "loss: 0.012660671364177357\tacc: 0.722024233784747\n",
            "(0.647502356267672, 0.9772403982930299)\n",
            "TESTING: Loss: 0.5518193658855226 | Acc: 0.717607973421927 \t\n",
            "(0.6410256410256411, 0.9933774834437086)\n",
            "loss: 0.014066112312403593\tacc: 0.7198859586600143\n",
            "(0.644589552238806, 0.9829302987197724)\n",
            "TESTING: Loss: 0.5550344702270296 | Acc: 0.707641196013289 \t\n",
            "(0.6323529411764706, 0.9966887417218543)\n",
            "loss: 0.01195913011377508\tacc: 0.7170349251603706\n",
            "(0.6411439114391144, 0.9886201991465149)\n",
            "TESTING: Loss: 0.5485524584849676 | Acc: 0.7159468438538206 \t\n",
            "(0.6396588486140725, 0.9933774834437086)\n",
            "loss: 0.014532568779858675\tacc: 0.7170349251603706\n",
            "(0.6432584269662921, 0.9772403982930299)\n",
            "TESTING: Loss: 0.544908924235238 | Acc: 0.717607973421927 \t\n",
            "(0.6416309012875536, 0.9900662251655629)\n",
            "loss: 0.012185809287157926\tacc: 0.7227369921596579\n",
            "(0.6472795497185742, 0.9815078236130867)\n",
            "TESTING: Loss: 0.5479467461506525 | Acc: 0.717607973421927 \t\n",
            "(0.6410256410256411, 0.9933774834437086)\n",
            "loss: 0.010295584797859192\tacc: 0.7213114754098361\n",
            "(0.6444444444444445, 0.9900426742532006)\n",
            "TESTING: Loss: 0.5523951152960459 | Acc: 0.707641196013289 \t\n",
            "(0.6323529411764706, 0.9966887417218543)\n",
            "loss: 0.012706795876676386\tacc: 0.7234497505345688\n",
            "(0.6459684893419834, 0.9914651493598862)\n",
            "TESTING: Loss: 0.5455596182081435 | Acc: 0.7192691029900332 \t\n",
            "(0.6423982869379015, 0.9933774834437086)\n",
            "loss: 0.0131141258911653\tacc: 0.7205987170349252\n",
            "(0.6454630495790459, 0.9815078236130867)\n",
            "TESTING: Loss: 0.553742852475908 | Acc: 0.707641196013289 \t\n",
            "(0.6323529411764706, 0.9966887417218543)\n",
            "********** \tTraing new model on class: 7 \t **********\n",
            "loss: 0.01372997598214583\tacc: 0.6197691197691197\n",
            "(0.5658657829328915, 0.9956268221574344)\n",
            "TESTING: Loss: 0.6396109163761139 | Acc: 0.702020202020202 \t\n",
            "(0.6247334754797441, 0.9965986394557823)\n",
            "loss: 0.011780436743389477\tacc: 0.7236652236652237\n",
            "(0.6478048780487805, 0.967930029154519)\n",
            "TESTING: Loss: 0.5628455844190385 | Acc: 0.7070707070707071 \t\n",
            "(0.628755364806867, 0.9965986394557823)\n",
            "loss: 0.00606691837310791\tacc: 0.7308802308802309\n",
            "(0.6500479386385427, 0.9883381924198251)\n",
            "TESTING: Loss: 0.5582178831100464 | Acc: 0.7087542087542088 \t\n",
            "(0.6301075268817204, 0.9965986394557823)\n",
            "loss: 0.00829213722185655\tacc: 0.7380952380952381\n",
            "(0.6551392891450528, 0.9941690962099126)\n",
            "TESTING: Loss: 0.5508052806059519 | Acc: 0.7154882154882155 \t\n",
            "(0.6355748373101953, 0.9965986394557823)\n",
            "loss: 0.015644368800249966\tacc: 0.7366522366522367\n",
            "(0.6556741028128031, 0.9854227405247813)\n",
            "TESTING: Loss: 0.5497211698028777 | Acc: 0.7154882154882155 \t\n",
            "(0.6355748373101953, 0.9965986394557823)\n",
            "loss: 0.011044475165280428\tacc: 0.7380952380952381\n",
            "(0.6548418024928092, 0.9956268221574344)\n",
            "TESTING: Loss: 0.5487312757306628 | Acc: 0.7171717171717171 \t\n",
            "(0.6369565217391304, 0.9965986394557823)\n",
            "loss: 0.010748412121425976\tacc: 0.7453102453102453\n",
            "(0.6614936954413191, 0.9941690962099126)\n",
            "TESTING: Loss: 0.5477868268887202 | Acc: 0.7239057239057239 \t\n",
            "(0.6425438596491229, 0.9965986394557823)\n",
            "loss: 0.010253146290779114\tacc: 0.7373737373737373\n",
            "(0.657843137254902, 0.978134110787172)\n",
            "TESTING: Loss: 0.5501583615938822 | Acc: 0.7222222222222222 \t\n",
            "(0.6411378555798687, 0.9965986394557823)\n",
            "loss: 0.012880865823138844\tacc: 0.7431457431457431\n",
            "(0.6592664092664092, 0.9956268221574344)\n",
            "TESTING: Loss: 0.5481893652015262 | Acc: 0.7239057239057239 \t\n",
            "(0.6425438596491229, 0.9965986394557823)\n",
            "loss: 0.0119842683727091\tacc: 0.740981240981241\n",
            "(0.6573628488931665, 0.9956268221574344)\n",
            "TESTING: Loss: 0.5461807250976562 | Acc: 0.7255892255892256 \t\n",
            "(0.643956043956044, 0.9965986394557823)\n",
            "********** \tTraing new model on class: 8 \t **********\n",
            "loss: 0.0135089172558351\tacc: 0.6194563662374821\n",
            "(0.5785984848484849, 0.8753581661891118)\n",
            "TESTING: Loss: 0.6642527812057071 | Acc: 0.6866666666666666 \t\n",
            "(0.6147540983606558, 1.0)\n",
            "loss: 0.013456127860329369\tacc: 0.6881258941344778\n",
            "(0.6224299065420561, 0.9541547277936963)\n",
            "TESTING: Loss: 0.5710197918944888 | Acc: 0.6983333333333334 \t\n",
            "(0.6237006237006237, 1.0)\n",
            "loss: 0.015244326808235863\tacc: 0.6952789699570815\n",
            "(0.628060263653484, 0.9555873925501432)\n",
            "TESTING: Loss: 0.5613188031646941 | Acc: 0.7066666666666667 \t\n",
            "(0.6302521008403361, 1.0)\n",
            "loss: 0.012394059788097034\tacc: 0.7081545064377682\n",
            "(0.6337638376383764, 0.9842406876790831)\n",
            "TESTING: Loss: 0.5601323544979095 | Acc: 0.7083333333333334 \t\n",
            "(0.631578947368421, 1.0)\n",
            "loss: 0.010134464637799696\tacc: 0.7067238912732475\n",
            "(0.6335807050092764, 0.9785100286532952)\n",
            "TESTING: Loss: 0.5579544984632068 | Acc: 0.7083333333333334 \t\n",
            "(0.631578947368421, 1.0)\n",
            "loss: 0.012943821874531832\tacc: 0.7017167381974249\n",
            "(0.6292548298068077, 0.9799426934097422)\n",
            "TESTING: Loss: 0.5611821595165465 | Acc: 0.7066666666666667 \t\n",
            "(0.6302521008403361, 1.0)\n",
            "loss: 0.01376819068735296\tacc: 0.7052932761087267\n",
            "(0.6319188191881919, 0.9813753581661891)\n",
            "TESTING: Loss: 0.5529441469245486 | Acc: 0.7083333333333334 \t\n",
            "(0.631578947368421, 1.0)\n",
            "loss: 0.010406921533021059\tacc: 0.7017167381974249\n",
            "(0.6287809349220899, 0.9828080229226361)\n",
            "TESTING: Loss: 0.5595823062790765 | Acc: 0.7066666666666667 \t\n",
            "(0.6302521008403361, 1.0)\n",
            "loss: 0.01303519444032149\tacc: 0.7060085836909872\n",
            "(0.6305732484076433, 0.9928366762177651)\n",
            "TESTING: Loss: 0.5509453697337044 | Acc: 0.7116666666666667 \t\n",
            "(0.6342494714587738, 1.0)\n",
            "loss: 0.012823153625835072\tacc: 0.7088698140200286\n",
            "(0.6328767123287671, 0.9928366762177651)\n",
            "TESTING: Loss: 0.5458954224983851 | Acc: 0.72 \t\n",
            "(0.6416309012875536, 0.9966666666666667)\n",
            "********** \tTraing new model on class: 9 \t **********\n",
            "loss: 0.013440420681780035\tacc: 0.6695526695526696\n",
            "(0.6197478991596639, 0.8600583090379009)\n",
            "TESTING: Loss: 0.6195480012231402 | Acc: 0.7171717171717171 \t\n",
            "(0.6363636363636364, 1.0)\n",
            "loss: 0.012147190895947542\tacc: 0.7128427128427128\n",
            "(0.6409001956947162, 0.9548104956268222)\n",
            "TESTING: Loss: 0.5071931613816155 | Acc: 0.7592592592592593 \t\n",
            "(0.674364896073903, 0.9931972789115646)\n",
            "loss: 0.011429059234532442\tacc: 0.7200577200577201\n",
            "(0.6469428007889546, 0.956268221574344)\n",
            "TESTING: Loss: 0.500760073463122 | Acc: 0.7727272727272727 \t\n",
            "(0.6870588235294117, 0.9931972789115646)\n",
            "loss: 0.012130871415138245\tacc: 0.7359307359307359\n",
            "(0.6553398058252428, 0.9839650145772595)\n",
            "TESTING: Loss: 0.49457841118176776 | Acc: 0.7693602693602694 \t\n",
            "(0.6838407494145199, 0.9931972789115646)\n",
            "loss: 0.010941428217020903\tacc: 0.7294372294372294\n",
            "(0.6517073170731708, 0.9737609329446064)\n",
            "TESTING: Loss: 0.49075933463043636 | Acc: 0.765993265993266 \t\n",
            "(0.6806526806526807, 0.9931972789115646)\n",
            "loss: 0.010948712175542658\tacc: 0.7337662337662337\n",
            "(0.6546341463414634, 0.978134110787172)\n",
            "TESTING: Loss: 0.49047304193178815 | Acc: 0.7727272727272727 \t\n",
            "(0.6870588235294117, 0.9931972789115646)\n",
            "loss: 0.010255681520158594\tacc: 0.7366522366522367\n",
            "(0.6550724637681159, 0.9883381924198251)\n",
            "TESTING: Loss: 0.48753276301754844 | Acc: 0.7727272727272727 \t\n",
            "(0.6870588235294117, 0.9931972789115646)\n",
            "loss: 0.00983018766749989\tacc: 0.7323232323232324\n",
            "(0.6521739130434783, 0.9839650145772595)\n",
            "TESTING: Loss: 0.4898560659752952 | Acc: 0.765993265993266 \t\n",
            "(0.6806526806526807, 0.9931972789115646)\n",
            "loss: 0.00938806411894885\tacc: 0.733044733044733\n",
            "(0.6539961013645225, 0.978134110787172)\n",
            "TESTING: Loss: 0.4901919696066115 | Acc: 0.7727272727272727 \t\n",
            "(0.6870588235294117, 0.9931972789115646)\n",
            "loss: 0.016078328544443302\tacc: 0.7380952380952381\n",
            "(0.6551392891450528, 0.9941690962099126)\n",
            "TESTING: Loss: 0.48614975810050964 | Acc: 0.7727272727272727 \t\n",
            "(0.6870588235294117, 0.9931972789115646)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW966DAVn2hF",
        "colab_type": "code",
        "outputId": "d9384e8a-ca99-436e-a3aa-9ab3341af665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "mab.test_all_classes(class_num=10, dataloader_dic=test_dls)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fcc88>\n",
            "TESTING: Loss: 0.5267709460523393 | Acc: 0.735 \t\n",
            "(0.6556291390728477, 0.99)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc898>\n",
            "TESTING: Loss: 0.5374070554971695 | Acc: 0.7265238879736409 \t\n",
            "(0.6496815286624203, 0.996742671009772)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc940>\n",
            "TESTING: Loss: 0.3775183591577742 | Acc: 0.8484349258649094 \t\n",
            "(0.7694235588972431, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc4a8>\n",
            "TESTING: Loss: 0.36069291333357495 | Acc: 0.8592964824120602 \t\n",
            "(0.7855227882037533, 0.9865319865319865)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc2e8>\n",
            "TESTING: Loss: 0.42068976660569507 | Acc: 0.8193979933110368 \t\n",
            "(0.7375, 0.9899328859060402)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fccf8>\n",
            "TESTING: Loss: 0.4210546496841643 | Acc: 0.8162251655629139 \t\n",
            "(0.7336561743341404, 0.9967105263157895)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc748>\n",
            "TESTING: Loss: 0.553742852475908 | Acc: 0.707641196013289 \t\n",
            "(0.6323529411764706, 0.9966887417218543)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc908>\n",
            "TESTING: Loss: 0.5461807250976562 | Acc: 0.7255892255892256 \t\n",
            "(0.643956043956044, 0.9965986394557823)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc5f8>\n",
            "TESTING: Loss: 0.5458954224983851 | Acc: 0.72 \t\n",
            "(0.6416309012875536, 0.9966666666666667)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179fc2b0>\n",
            "TESTING: Loss: 0.48614975810050964 | Acc: 0.7727272727272727 \t\n",
            "(0.6870588235294117, 0.9931972789115646)\n",
            "TESTING precision: 0.6894481643962134, recall: 0.9943389943389943\n",
            "confusion matrix:\n",
            "TN: 1655\tFP: 1345\n",
            "FN: 17\t TP:2986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU6-TDtks-me",
        "colab_type": "code",
        "outputId": "21a11e89-317b-4c07-e1ef-293f006c63ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "source": [
        "class_id = list(range(0, 10))\n",
        "plt.xticks(range(0, 10))\n",
        "plt.yticks(np.arange(0, 1, 0.1))\n",
        "plt.ylim([0,1])\n",
        "plt.plot(class_id, test_accs, '*-', label='test acc')\n",
        "plt.plot(class_id, attack_precision, 'o-', label='attack precision')\n",
        "plt.title('target model\\'s test acc and attack precision per class')\n",
        "plt.xlabel('class id')\n",
        "plt.ylabel('acc')\n",
        "plt.grid('on')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "class_id = list(range(0, 10))\n",
        "plt.xticks(range(0, 10))\n",
        "plt.yticks(np.arange(0, 1, 0.1))\n",
        "plt.ylim([0,1])\n",
        "plt.plot(class_id, test_accs, '*-', label='test acc')\n",
        "plt.plot(class_id, attack_recall, 'o-', label='attack recall')\n",
        "plt.title('target model\\'s test acc and attack recall per class')\n",
        "plt.xlabel('class id')\n",
        "plt.ylabel('acc')\n",
        "plt.grid('on')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERHRkqIQEREtKQoREdHyX004rdwxFd2eAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADU4BjIcMAF2",
        "colab_type": "text"
      },
      "source": [
        "# difference in recall for cifar-10 \n",
        "\n",
        "shokri: always 1\n",
        "\n",
        "ML-Leaks: precision: 0.8, 0.8, sorted input x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrmcmBXuuPxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# a simple mutiperception attack model in ML-Leaks\n",
        "\n",
        "class attackModel1(nn.Module):\n",
        "    def __init__(self, num_in_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_in_features, 64)\n",
        "        self.fc2 = nn.Linear(64,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "#         x, _ = torch.sort(x, descending=True)\n",
        "#         print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKRz7vj8Yl32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class attackModel2(nn.Module):\n",
        "    def __init__(self, num_in_features):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_in_features, 128)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(64,64)\n",
        "        self.fc4 = nn.Linear(64,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = torch.topk(x, k=3, dim=-1)\n",
        "        x = self.dropout1(F.relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.relu(self.fc2(x)))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXvgG5DUM5xZ",
        "colab_type": "code",
        "outputId": "71e53422-5273-42c4-ded4-f1e8b0abba76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mab1 = AttackModelBundle(10, Model=attackModel1)\n",
        "\n",
        "for i in range(10):\n",
        "  mab1.train_class_model(i, train_dls[i], test_dls[i],  epoch=20)\n",
        "  \n",
        "mab1.test_all_classes(class_num=10, dataloader_dic=test_dls)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********** \tTraing new model on class: 0 \t **********\n",
            "loss: 0.014791210944002325\tacc: 0.6113879003558719\n",
            "(0.6480446927374302, 0.49361702127659574)\n",
            "TESTING: Loss: 0.6857075360086229 | Acc: 0.6865671641791045 \t\n",
            "(0.6158536585365854, 1.0)\n",
            "loss: 0.013347746296362444\tacc: 0.7103202846975089\n",
            "(0.6339928057553957, 1.0)\n",
            "TESTING: Loss: 0.6480261915259891 | Acc: 0.693200663349917 \t\n",
            "(0.6209016393442623, 1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.013707523996179754\tacc: 0.7153024911032029\n",
            "(0.6380090497737556, 1.0)\n",
            "TESTING: Loss: 0.6170860694514381 | Acc: 0.6965174129353234 \t\n",
            "(0.6234567901234568, 1.0)\n",
            "loss: 0.01232588155703111\tacc: 0.7195729537366548\n",
            "(0.6414922656960873, 1.0)\n",
            "TESTING: Loss: 0.5959451546271642 | Acc: 0.6998341625207297 \t\n",
            "(0.6260330578512396, 1.0)\n",
            "loss: 0.012602233073928139\tacc: 0.7195729537366548\n",
            "(0.6414922656960873, 1.0)\n",
            "TESTING: Loss: 0.5826833579275343 | Acc: 0.7014925373134329 \t\n",
            "(0.6273291925465838, 1.0)\n",
            "loss: 0.010975931178439747\tacc: 0.7245551601423488\n",
            "(0.6456043956043956, 1.0)\n",
            "TESTING: Loss: 0.5742156853278478 | Acc: 0.703150912106136 \t\n",
            "(0.6286307053941909, 1.0)\n",
            "loss: 0.009934997694058851\tacc: 0.7288256227758008\n",
            "(0.649171270718232, 1.0)\n",
            "TESTING: Loss: 0.5693474676873949 | Acc: 0.7048092868988391 \t\n",
            "(0.6299376299376299, 1.0)\n",
            "loss: 0.013588703491471031\tacc: 0.7288256227758008\n",
            "(0.649171270718232, 1.0)\n",
            "TESTING: Loss: 0.5661070677969191 | Acc: 0.7048092868988391 \t\n",
            "(0.6299376299376299, 1.0)\n",
            "loss: 0.01126749881289222\tacc: 0.7302491103202847\n",
            "(0.6503690036900369, 1.0)\n",
            "TESTING: Loss: 0.5627877778477139 | Acc: 0.7081260364842454 \t\n",
            "(0.6325678496868476, 1.0)\n",
            "loss: 0.013000844554467634\tacc: 0.7302491103202847\n",
            "(0.6503690036900369, 1.0)\n",
            "TESTING: Loss: 0.5621496339639028 | Acc: 0.7048092868988391 \t\n",
            "(0.6299376299376299, 1.0)\n",
            "loss: 0.00848576697436246\tacc: 0.7330960854092526\n",
            "(0.6527777777777778, 1.0)\n",
            "TESTING: Loss: 0.5599345316489538 | Acc: 0.7081260364842454 \t\n",
            "(0.6325678496868476, 1.0)\n",
            "loss: 0.011376510966907848\tacc: 0.7309608540925266\n",
            "(0.6509695290858726, 1.0)\n",
            "TESTING: Loss: 0.5572408007250892 | Acc: 0.7081260364842454 \t\n",
            "(0.6325678496868476, 1.0)\n",
            "loss: 0.012315928936004639\tacc: 0.7352313167259786\n",
            "(0.6545961002785515, 1.0)\n",
            "TESTING: Loss: 0.5564779043197632 | Acc: 0.7081260364842454 \t\n",
            "(0.6325678496868476, 1.0)\n",
            "loss: 0.011191643097183922\tacc: 0.7323843416370107\n",
            "(0.6521739130434783, 1.0)\n",
            "TESTING: Loss: 0.5550215740998586 | Acc: 0.714759535655058 \t\n",
            "(0.6378947368421053, 1.0)\n",
            "loss: 0.011100017888979479\tacc: 0.7345195729537367\n",
            "(0.6539888682745826, 1.0)\n",
            "TESTING: Loss: 0.5568515807390213 | Acc: 0.7081260364842454 \t\n",
            "(0.6325678496868476, 1.0)\n",
            "loss: 0.010788142003796318\tacc: 0.7366548042704626\n",
            "(0.6558139534883721, 1.0)\n",
            "TESTING: Loss: 0.5535096079111099 | Acc: 0.7131011608623549 \t\n",
            "(0.6365546218487395, 1.0)\n",
            "loss: 0.011553053151477467\tacc: 0.7352313167259786\n",
            "(0.6545961002785515, 1.0)\n",
            "TESTING: Loss: 0.5542859368854098 | Acc: 0.7097844112769486 \t\n",
            "(0.6338912133891214, 1.0)\n",
            "loss: 0.009901318360458721\tacc: 0.7352313167259786\n",
            "(0.6545961002785515, 1.0)\n",
            "TESTING: Loss: 0.5523311843474706 | Acc: 0.714759535655058 \t\n",
            "(0.6378947368421053, 1.0)\n",
            "loss: 0.01155303960496729\tacc: 0.7359430604982207\n",
            "(0.6552044609665427, 1.0)\n",
            "TESTING: Loss: 0.5525026238626904 | Acc: 0.7131011608623549 \t\n",
            "(0.6365546218487395, 1.0)\n",
            "loss: 0.009337617592378096\tacc: 0.7380782918149467\n",
            "(0.6570363466915191, 1.0)\n",
            "TESTING: Loss: 0.5517809357908037 | Acc: 0.714759535655058 \t\n",
            "(0.6378947368421053, 1.0)\n",
            "********** \tTraing new model on class: 1 \t **********\n",
            "loss: 0.0133430282274882\tacc: 0.6351446718419196\n",
            "(0.5810372771474879, 1.0)\n",
            "TESTING: Loss: 0.6825091342131296 | Acc: 0.6973684210526315 \t\n",
            "(0.6260162601626016, 1.0)\n",
            "loss: 0.012019271320766873\tacc: 0.6824276640790402\n",
            "(0.6143958868894601, 1.0)\n",
            "TESTING: Loss: 0.6458505094051361 | Acc: 0.7072368421052632 \t\n",
            "(0.6337448559670782, 1.0)\n",
            "loss: 0.015250757005479601\tacc: 0.6866619618913197\n",
            "(0.6175710594315246, 1.0)\n",
            "TESTING: Loss: 0.6153867575857375 | Acc: 0.7072368421052632 \t\n",
            "(0.6337448559670782, 1.0)\n",
            "loss: 0.009388099776373969\tacc: 0.6908962597035991\n",
            "(0.6207792207792208, 1.0)\n",
            "TESTING: Loss: 0.5928729938136207 | Acc: 0.7072368421052632 \t\n",
            "(0.6337448559670782, 1.0)\n",
            "loss: 0.012674355506896972\tacc: 0.6908962597035991\n",
            "(0.6207792207792208, 1.0)\n",
            "TESTING: Loss: 0.5785434593756994 | Acc: 0.7088815789473685 \t\n",
            "(0.6350515463917525, 1.0)\n",
            "loss: 0.012278408474392362\tacc: 0.6923076923076923\n",
            "(0.6218560277536861, 1.0)\n",
            "TESTING: Loss: 0.57050102783574 | Acc: 0.7154605263157895 \t\n",
            "(0.6403326403326404, 1.0)\n",
            "loss: 0.014179839028252495\tacc: 0.6930134086097389\n",
            "(0.6223958333333334, 1.0)\n",
            "TESTING: Loss: 0.5649758991267946 | Acc: 0.7154605263157895 \t\n",
            "(0.6403326403326404, 1.0)\n",
            "loss: 0.011945296658409967\tacc: 0.6972477064220184\n",
            "(0.6256544502617801, 1.0)\n",
            "TESTING: Loss: 0.5600926809840732 | Acc: 0.7154605263157895 \t\n",
            "(0.6403326403326404, 1.0)\n",
            "loss: 0.014273851447635226\tacc: 0.6972477064220184\n",
            "(0.6256544502617801, 1.0)\n",
            "TESTING: Loss: 0.557532830370797 | Acc: 0.7154605263157895 \t\n",
            "(0.6403326403326404, 1.0)\n",
            "loss: 0.01414911217159695\tacc: 0.6986591390261115\n",
            "(0.6267482517482518, 1.0)\n",
            "TESTING: Loss: 0.5562693145540025 | Acc: 0.71875 \t\n",
            "(0.6430062630480167, 1.0)\n",
            "loss: 0.00877223147286309\tacc: 0.7000705716302047\n",
            "(0.62784588441331, 1.0)\n",
            "TESTING: Loss: 0.5531620962752236 | Acc: 0.71875 \t\n",
            "(0.6430062630480167, 1.0)\n",
            "loss: 0.014337742328643798\tacc: 0.6993648553281581\n",
            "(0.6272965879265092, 1.0)\n",
            "TESTING: Loss: 0.5514195528295305 | Acc: 0.71875 \t\n",
            "(0.6430062630480167, 1.0)\n",
            "loss: 0.015406453609466552\tacc: 0.6993648553281581\n",
            "(0.6272965879265092, 1.0)\n",
            "TESTING: Loss: 0.5509211156103346 | Acc: 0.7203947368421053 \t\n",
            "(0.6443514644351465, 1.0)\n",
            "loss: 0.010696506500244141\tacc: 0.7007762879322512\n",
            "(0.628396143733567, 1.0)\n",
            "TESTING: Loss: 0.5501496841510137 | Acc: 0.7203947368421053 \t\n",
            "(0.6443514644351465, 1.0)\n",
            "loss: 0.009008675151401095\tacc: 0.7043048694424842\n",
            "(0.6311619718309859, 1.0)\n",
            "TESTING: Loss: 0.5478050112724304 | Acc: 0.7171052631578947 \t\n",
            "(0.6416666666666667, 1.0)\n",
            "loss: 0.013084031475914849\tacc: 0.7000705716302047\n",
            "(0.62784588441331, 1.0)\n",
            "TESTING: Loss: 0.5475234372748269 | Acc: 0.7203947368421053 \t\n",
            "(0.6443514644351465, 1.0)\n",
            "loss: 0.010778058899773492\tacc: 0.702893436838391\n",
            "(0.6300527240773286, 1.0)\n",
            "TESTING: Loss: 0.5469174202945497 | Acc: 0.7236842105263158 \t\n",
            "(0.6470588235294118, 1.0)\n",
            "loss: 0.011976406309339735\tacc: 0.7035991531404375\n",
            "(0.6306068601583114, 1.0)\n",
            "TESTING: Loss: 0.5455449985133277 | Acc: 0.7203947368421053 \t\n",
            "(0.6443514644351465, 1.0)\n",
            "loss: 0.0072143826219770646\tacc: 0.7021877205363444\n",
            "(0.6294995610184372, 1.0)\n",
            "TESTING: Loss: 0.5464902503622903 | Acc: 0.725328947368421 \t\n",
            "(0.6484210526315789, 1.0)\n",
            "loss: 0.008285852273305257\tacc: 0.7057163020465773\n",
            "(0.6322751322751323, 1.0)\n",
            "TESTING: Loss: 0.5443261480993695 | Acc: 0.7220394736842105 \t\n",
            "(0.6457023060796646, 1.0)\n",
            "********** \tTraing new model on class: 2 \t **********\n",
            "loss: 0.013900517031203869\tacc: 0.7707423580786026\n",
            "(0.6814964610717897, 1.0)\n",
            "TESTING: Loss: 0.6111838784482744 | Acc: 0.7928692699490663 \t\n",
            "(0.7031630170316302, 1.0)\n",
            "loss: 0.01092107143512992\tacc: 0.7998544395924309\n",
            "(0.7102212855637513, 1.0)\n",
            "TESTING: Loss: 0.5396438522471322 | Acc: 0.799660441426146 \t\n",
            "(0.7100737100737101, 1.0)\n",
            "loss: 0.012754326642945756\tacc: 0.8056768558951966\n",
            "(0.7162592986184909, 1.0)\n",
            "TESTING: Loss: 0.4908000843392478 | Acc: 0.8064516129032258 \t\n",
            "(0.71712158808933, 1.0)\n",
            "loss: 0.011098497828771902\tacc: 0.8093158660844251\n",
            "(0.7200854700854701, 1.0)\n",
            "TESTING: Loss: 0.4615754700369305 | Acc: 0.8115449915110357 \t\n",
            "(0.7225, 1.0)\n",
            "loss: 0.007548821526904439\tacc: 0.8151382823871907\n",
            "(0.7262931034482759, 1.0)\n",
            "TESTING: Loss: 0.44656037125322556 | Acc: 0.8115449915110357 \t\n",
            "(0.7225, 1.0)\n",
            "loss: 0.010002428709074508\tacc: 0.8158660844250364\n",
            "(0.727076591154261, 1.0)\n",
            "TESTING: Loss: 0.4365328964259889 | Acc: 0.8132427843803056 \t\n",
            "(0.7243107769423559, 1.0)\n",
            "loss: 0.008899047624233157\tacc: 0.8165938864628821\n",
            "(0.7278617710583153, 1.0)\n",
            "TESTING: Loss: 0.4304649879535039 | Acc: 0.8166383701188455 \t\n",
            "(0.7279596977329975, 1.0)\n",
            "loss: 0.010911107756370721\tacc: 0.8165938864628821\n",
            "(0.7278617710583153, 1.0)\n",
            "TESTING: Loss: 0.42649679217073655 | Acc: 0.8166383701188455 \t\n",
            "(0.7279596977329975, 1.0)\n",
            "loss: 0.009936956472175067\tacc: 0.8173216885007278\n",
            "(0.7286486486486486, 1.0)\n",
            "TESTING: Loss: 0.42423990699979996 | Acc: 0.8166383701188455 \t\n",
            "(0.7279596977329975, 1.0)\n",
            "loss: 0.010998014793839566\tacc: 0.8180494905385735\n",
            "(0.7294372294372294, 1.0)\n",
            "TESTING: Loss: 0.42215676108996075 | Acc: 0.8166383701188455 \t\n",
            "(0.7279596977329975, 1.0)\n",
            "loss: 0.011043528484743695\tacc: 0.8180494905385735\n",
            "(0.7294372294372294, 1.0)\n",
            "TESTING: Loss: 0.4193735486931271 | Acc: 0.8166383701188455 \t\n",
            "(0.7279596977329975, 1.0)\n",
            "loss: 0.012616228225619294\tacc: 0.8195050946142649\n",
            "(0.7310195227765727, 1.0)\n",
            "TESTING: Loss: 0.41976333492332035 | Acc: 0.8166383701188455 \t\n",
            "(0.7279596977329975, 1.0)\n",
            "loss: 0.010287051977113236\tacc: 0.8209606986899564\n",
            "(0.7326086956521739, 1.0)\n",
            "TESTING: Loss: 0.4172762864165836 | Acc: 0.8183361629881154 \t\n",
            "(0.7297979797979798, 1.0)\n",
            "loss: 0.013219831987868908\tacc: 0.8202328966521106\n",
            "(0.7318132464712269, 1.0)\n",
            "TESTING: Loss: 0.41594913767443764 | Acc: 0.8183361629881154 \t\n",
            "(0.7297979797979798, 1.0)\n",
            "loss: 0.012472960838051729\tacc: 0.8216885007278021\n",
            "(0.7334058759521219, 1.0)\n",
            "TESTING: Loss: 0.41522521442837185 | Acc: 0.8183361629881154 \t\n",
            "(0.7297979797979798, 1.0)\n",
            "loss: 0.011101478754087936\tacc: 0.8231441048034934\n",
            "(0.7350054525627044, 1.0)\n",
            "TESTING: Loss: 0.41357816921340096 | Acc: 0.8183361629881154 \t\n",
            "(0.7297979797979798, 1.0)\n",
            "loss: 0.008012242788492246\tacc: 0.8231441048034934\n",
            "(0.7350054525627044, 1.0)\n",
            "TESTING: Loss: 0.4123256487978829 | Acc: 0.8200339558573854 \t\n",
            "(0.7316455696202532, 1.0)\n",
            "loss: 0.008424392966336982\tacc: 0.8231441048034934\n",
            "(0.7350054525627044, 1.0)\n",
            "TESTING: Loss: 0.4115913096401427 | Acc: 0.8200339558573854 \t\n",
            "(0.7316455696202532, 1.0)\n",
            "loss: 0.012406911960867949\tacc: 0.8231441048034934\n",
            "(0.7350054525627044, 1.0)\n",
            "TESTING: Loss: 0.41092144946257275 | Acc: 0.8200339558573854 \t\n",
            "(0.7316455696202532, 1.0)\n",
            "loss: 0.008458782767140589\tacc: 0.8245997088791849\n",
            "(0.7366120218579235, 1.0)\n",
            "TESTING: Loss: 0.4110036740700404 | Acc: 0.8200339558573854 \t\n",
            "(0.7316455696202532, 1.0)\n",
            "********** \tTraing new model on class: 3 \t **********\n",
            "loss: 0.013336451848347981\tacc: 0.745249824067558\n",
            "(0.7567954220314735, 0.7337031900138696)\n",
            "TESTING: Loss: 0.6296761945674294 | Acc: 0.8180327868852459 \t\n",
            "(0.7363420427553444, 1.0)\n",
            "loss: 0.00971676508585612\tacc: 0.8367346938775511\n",
            "(0.7565582371458552, 1.0)\n",
            "TESTING: Loss: 0.5483026284920541 | Acc: 0.8229508196721311 \t\n",
            "(0.7416267942583732, 1.0)\n",
            "loss: 0.009012091822094387\tacc: 0.8423645320197044\n",
            "(0.762962962962963, 1.0)\n",
            "TESTING: Loss: 0.4835168813404284 | Acc: 0.8295081967213115 \t\n",
            "(0.748792270531401, 1.0)\n",
            "loss: 0.006537002325057983\tacc: 0.8451794510907812\n",
            "(0.7662061636556854, 1.0)\n",
            "TESTING: Loss: 0.44831856300956324 | Acc: 0.8311475409836065 \t\n",
            "(0.7506053268765133, 1.0)\n",
            "loss: 0.007871038383907743\tacc: 0.8479943701618579\n",
            "(0.7694770544290288, 1.0)\n",
            "TESTING: Loss: 0.4316256626656181 | Acc: 0.8327868852459016 \t\n",
            "(0.7524271844660194, 1.0)\n",
            "loss: 0.00986704428990682\tacc: 0.8501055594651654\n",
            "(0.771948608137045, 1.0)\n",
            "TESTING: Loss: 0.41901938695656626 | Acc: 0.839344262295082 \t\n",
            "(0.7598039215686274, 1.0)\n",
            "loss: 0.007955823342005413\tacc: 0.8508092892329345\n",
            "(0.7727759914255091, 1.0)\n",
            "TESTING: Loss: 0.4123453356717762 | Acc: 0.839344262295082 \t\n",
            "(0.7598039215686274, 1.0)\n",
            "loss: 0.0038587676154242623\tacc: 0.8536242083040113\n",
            "(0.7761033369214209, 1.0)\n",
            "TESTING: Loss: 0.40745882768380015 | Acc: 0.8426229508196721 \t\n",
            "(0.7635467980295566, 1.0)\n",
            "loss: 0.008625821272532145\tacc: 0.8550316678395496\n",
            "(0.7777777777777778, 1.0)\n",
            "TESTING: Loss: 0.40551031106396723 | Acc: 0.8426229508196721 \t\n",
            "(0.7635467980295566, 1.0)\n",
            "loss: 0.00457909901936849\tacc: 0.8557353976073188\n",
            "(0.7786177105831533, 1.0)\n",
            "TESTING: Loss: 0.4028433843662864 | Acc: 0.8426229508196721 \t\n",
            "(0.7635467980295566, 1.0)\n",
            "loss: 0.004639767275916205\tacc: 0.8571428571428571\n",
            "(0.7803030303030303, 1.0)\n",
            "TESTING: Loss: 0.39982523416218 | Acc: 0.8442622950819673 \t\n",
            "(0.7654320987654321, 1.0)\n",
            "loss: 0.007057060135735406\tacc: 0.8592540464461647\n",
            "(0.7828447339847991, 1.0)\n",
            "TESTING: Loss: 0.401207759976387 | Acc: 0.8442622950819673 \t\n",
            "(0.7654320987654321, 1.0)\n",
            "loss: 0.009728813171386718\tacc: 0.8592540464461647\n",
            "(0.7828447339847991, 1.0)\n",
            "TESTING: Loss: 0.3982622184251484 | Acc: 0.8475409836065574 \t\n",
            "(0.7692307692307693, 1.0)\n",
            "loss: 0.006748872995376587\tacc: 0.8599577762139339\n",
            "(0.783695652173913, 1.0)\n",
            "TESTING: Loss: 0.3968867290961115 | Acc: 0.8475409836065574 \t\n",
            "(0.7692307692307693, 1.0)\n",
            "loss: 0.010225422514809502\tacc: 0.8613652357494722\n",
            "(0.7854030501089324, 1.0)\n",
            "TESTING: Loss: 0.39518254367928757 | Acc: 0.8475409836065574 \t\n",
            "(0.7692307692307693, 1.0)\n",
            "loss: 0.007670851548512777\tacc: 0.8613652357494722\n",
            "(0.7854030501089324, 1.0)\n",
            "TESTING: Loss: 0.39501160540078817 | Acc: 0.8475409836065574 \t\n",
            "(0.7692307692307693, 1.0)\n",
            "loss: 0.0030787742800182766\tacc: 0.8634764250527798\n",
            "(0.7879781420765027, 1.0)\n",
            "TESTING: Loss: 0.39238691643664714 | Acc: 0.8491803278688524 \t\n",
            "(0.7711442786069652, 1.0)\n",
            "loss: 0.012713000509474012\tacc: 0.8634764250527798\n",
            "(0.7879781420765027, 1.0)\n",
            "TESTING: Loss: 0.39343702714694173 | Acc: 0.8491803278688524 \t\n",
            "(0.7711442786069652, 1.0)\n",
            "loss: 0.005603002177344428\tacc: 0.8648838845883181\n",
            "(0.7897042716319824, 1.0)\n",
            "TESTING: Loss: 0.3915400716819261 | Acc: 0.8491803278688524 \t\n",
            "(0.7711442786069652, 1.0)\n",
            "loss: 0.008925722704993354\tacc: 0.8634764250527798\n",
            "(0.7879781420765027, 1.0)\n",
            "TESTING: Loss: 0.3903703313124807 | Acc: 0.8491803278688524 \t\n",
            "(0.7711442786069652, 1.0)\n",
            "********** \tTraing new model on class: 4 \t **********\n",
            "loss: 0.01500098233999208\tacc: 0.6195014662756598\n",
            "(0.704225352112676, 0.37650602409638556)\n",
            "TESTING: Loss: 0.6604738864633772 | Acc: 0.764102564102564 \t\n",
            "(0.6737588652482269, 1.0)\n",
            "loss: 0.012859606465627981\tacc: 0.783724340175953\n",
            "(0.6923879040667362, 1.0)\n",
            "TESTING: Loss: 0.5949208935101827 | Acc: 0.7743589743589744 \t\n",
            "(0.6834532374100719, 1.0)\n",
            "loss: 0.015005524768385776\tacc: 0.7866568914956011\n",
            "(0.6952879581151833, 1.0)\n",
            "TESTING: Loss: 0.5373522341251373 | Acc: 0.7760683760683761 \t\n",
            "(0.6850961538461539, 1.0)\n",
            "loss: 0.011890321276908698\tacc: 0.7895894428152492\n",
            "(0.6982124079915878, 1.0)\n",
            "TESTING: Loss: 0.4982496549685796 | Acc: 0.7794871794871795 \t\n",
            "(0.6884057971014492, 1.0)\n",
            "loss: 0.009600432806236799\tacc: 0.7895894428152492\n",
            "(0.6982124079915878, 1.0)\n",
            "TESTING: Loss: 0.47572651588254505 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.012719554956569228\tacc: 0.7917888563049853\n",
            "(0.70042194092827, 1.0)\n",
            "TESTING: Loss: 0.4634830852349599 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.011521782985953398\tacc: 0.7917888563049853\n",
            "(0.70042194092827, 1.0)\n",
            "TESTING: Loss: 0.4560237104694049 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.008392859336941741\tacc: 0.7939882697947214\n",
            "(0.7026455026455026, 1.0)\n",
            "TESTING: Loss: 0.4509035398562749 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.010170971238335897\tacc: 0.7925219941348973\n",
            "(0.7011615628299894, 1.0)\n",
            "TESTING: Loss: 0.4477888494729996 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.015558988548988519\tacc: 0.7939882697947214\n",
            "(0.7026455026455026, 1.0)\n",
            "TESTING: Loss: 0.44627276187141734 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.004988517179045566\tacc: 0.7969208211143695\n",
            "(0.7056323060573858, 1.0)\n",
            "TESTING: Loss: 0.4428545157942507 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.008018054241357847\tacc: 0.7976539589442815\n",
            "(0.7063829787234043, 1.0)\n",
            "TESTING: Loss: 0.44133030043707955 | Acc: 0.788034188034188 \t\n",
            "(0.6968215158924206, 1.0)\n",
            "loss: 0.012802890566892402\tacc: 0.7983870967741935\n",
            "(0.7071352502662407, 1.0)\n",
            "TESTING: Loss: 0.44022682640287614 | Acc: 0.788034188034188 \t\n",
            "(0.6968215158924206, 1.0)\n",
            "loss: 0.011504666749821153\tacc: 0.7983870967741935\n",
            "(0.7071352502662407, 1.0)\n",
            "TESTING: Loss: 0.4398304203318225 | Acc: 0.7863247863247863 \t\n",
            "(0.6951219512195121, 1.0)\n",
            "loss: 0.009232813535734664\tacc: 0.8005865102639296\n",
            "(0.7094017094017094, 1.0)\n",
            "TESTING: Loss: 0.4380510503219234 | Acc: 0.7897435897435897 \t\n",
            "(0.6985294117647058, 1.0)\n",
            "loss: 0.008132609517075295\tacc: 0.7998533724340176\n",
            "(0.7086446104589115, 1.0)\n",
            "TESTING: Loss: 0.43665166199207306 | Acc: 0.7914529914529914 \t\n",
            "(0.7002457002457002, 1.0)\n",
            "loss: 0.009602197380953057\tacc: 0.8005865102639296\n",
            "(0.7094017094017094, 1.0)\n",
            "TESTING: Loss: 0.4355180035862658 | Acc: 0.7931623931623931 \t\n",
            "(0.7019704433497537, 1.0)\n",
            "loss: 0.008247001919635507\tacc: 0.8027859237536656\n",
            "(0.7116827438370846, 1.0)\n",
            "TESTING: Loss: 0.4357151786486308 | Acc: 0.7914529914529914 \t\n",
            "(0.7002457002457002, 1.0)\n",
            "loss: 0.013276344121888627\tacc: 0.8005865102639296\n",
            "(0.7094017094017094, 1.0)\n",
            "TESTING: Loss: 0.43384597409102654 | Acc: 0.7931623931623931 \t\n",
            "(0.7019704433497537, 1.0)\n",
            "loss: 0.011957814527112384\tacc: 0.8035190615835777\n",
            "(0.7124463519313304, 1.0)\n",
            "TESTING: Loss: 0.4334261119365692 | Acc: 0.7914529914529914 \t\n",
            "(0.7002457002457002, 1.0)\n",
            "********** \tTraing new model on class: 5 \t **********\n",
            "loss: 0.013563641092993996\tacc: 0.6891117478510028\n",
            "(0.6159292035398231, 1.0)\n",
            "TESTING: Loss: 0.6322764919863807 | Acc: 0.8046744574290484 \t\n",
            "(0.71875, 1.0)\n",
            "loss: 0.01098579845645211\tacc: 0.7915472779369628\n",
            "(0.7051671732522796, 1.0)\n",
            "TESTING: Loss: 0.5443142354488373 | Acc: 0.8297161936560935 \t\n",
            "(0.7456359102244389, 1.0)\n",
            "loss: 0.012594080784104088\tacc: 0.7965616045845272\n",
            "(0.710204081632653, 1.0)\n",
            "TESTING: Loss: 0.47943375011285144 | Acc: 0.8313856427378965 \t\n",
            "(0.7475, 1.0)\n",
            "loss: 0.00989474898034876\tacc: 0.7979942693409742\n",
            "(0.7116564417177914, 1.0)\n",
            "TESTING: Loss: 0.44426705771022373 | Acc: 0.8313856427378965 \t\n",
            "(0.7475, 1.0)\n",
            "loss: 0.008094810626723549\tacc: 0.8001432664756447\n",
            "(0.7138461538461538, 1.0)\n",
            "TESTING: Loss: 0.4258740610546536 | Acc: 0.8330550918196995 \t\n",
            "(0.7493734335839599, 1.0)\n",
            "loss: 0.011924842541868036\tacc: 0.8015759312320917\n",
            "(0.7153134635149023, 1.0)\n",
            "TESTING: Loss: 0.4144689655966229 | Acc: 0.8330550918196995 \t\n",
            "(0.7493734335839599, 1.0)\n",
            "loss: 0.00800483606078408\tacc: 0.8044412607449857\n",
            "(0.718266253869969, 1.0)\n",
            "TESTING: Loss: 0.40720152854919434 | Acc: 0.8330550918196995 \t\n",
            "(0.7493734335839599, 1.0)\n",
            "loss: 0.009005161171609705\tacc: 0.8037249283667621\n",
            "(0.7175257731958763, 1.0)\n",
            "TESTING: Loss: 0.40221477962202495 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.010961928150870583\tacc: 0.8051575931232091\n",
            "(0.71900826446281, 1.0)\n",
            "TESTING: Loss: 0.40153258045514423 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.01049242913722992\tacc: 0.8080229226361032\n",
            "(0.7219917012448133, 1.0)\n",
            "TESTING: Loss: 0.397255164053705 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.006678716025569222\tacc: 0.8080229226361032\n",
            "(0.7219917012448133, 1.0)\n",
            "TESTING: Loss: 0.39868030697107315 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.009518224407326092\tacc: 0.8087392550143266\n",
            "(0.7227414330218068, 1.0)\n",
            "TESTING: Loss: 0.3942714176244206 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.008727423169396141\tacc: 0.8087392550143266\n",
            "(0.7227414330218068, 1.0)\n",
            "TESTING: Loss: 0.39553776135047275 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.01031976193189621\tacc: 0.8094555873925502\n",
            "(0.7234927234927235, 1.0)\n",
            "TESTING: Loss: 0.3920274567272928 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.007542074403979562\tacc: 0.8080229226361032\n",
            "(0.7219917012448133, 1.0)\n",
            "TESTING: Loss: 0.3921429076128536 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.010910702699964697\tacc: 0.8101719197707736\n",
            "(0.7242455775234131, 1.0)\n",
            "TESTING: Loss: 0.3900326258606381 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.01460087705742229\tacc: 0.8108882521489972\n",
            "(0.725, 1.0)\n",
            "TESTING: Loss: 0.3894129271308581 | Acc: 0.8347245409015025 \t\n",
            "(0.7512562814070352, 1.0)\n",
            "loss: 0.008947499096393585\tacc: 0.8094555873925502\n",
            "(0.7234927234927235, 1.0)\n",
            "TESTING: Loss: 0.39339306122726864 | Acc: 0.8380634390651085 \t\n",
            "(0.7550505050505051, 1.0)\n",
            "loss: 0.007984132929281755\tacc: 0.8108882521489972\n",
            "(0.725, 1.0)\n",
            "TESTING: Loss: 0.3922612898879581 | Acc: 0.8380634390651085 \t\n",
            "(0.7550505050505051, 1.0)\n",
            "loss: 0.009232750670476393\tacc: 0.8108882521489972\n",
            "(0.725, 1.0)\n",
            "TESTING: Loss: 0.38785069270266426 | Acc: 0.8363939899833055 \t\n",
            "(0.7531486146095718, 1.0)\n",
            "********** \tTraing new model on class: 6 \t **********\n",
            "loss: 0.014277240965101454\tacc: 0.6167832167832168\n",
            "(0.6022471910112359, 0.7342465753424657)\n",
            "TESTING: Loss: 0.6784983747883847 | Acc: 0.7019543973941368 \t\n",
            "(0.6317907444668008, 1.0)\n",
            "loss: 0.01231032477484809\tacc: 0.6867132867132867\n",
            "(0.6196943972835314, 1.0)\n",
            "TESTING: Loss: 0.6391290049803885 | Acc: 0.7100977198697068 \t\n",
            "(0.6382113821138211, 1.0)\n",
            "loss: 0.012661116653018528\tacc: 0.6895104895104895\n",
            "(0.6218057921635435, 1.0)\n",
            "TESTING: Loss: 0.6072948355423776 | Acc: 0.7149837133550488 \t\n",
            "(0.6421267893660532, 1.0)\n",
            "loss: 0.012463907400767008\tacc: 0.6944055944055944\n",
            "(0.6255355612682091, 1.0)\n",
            "TESTING: Loss: 0.5858950803154394 | Acc: 0.7149837133550488 \t\n",
            "(0.6421267893660532, 1.0)\n",
            "loss: 0.009684090481864081\tacc: 0.6965034965034965\n",
            "(0.627147766323024, 1.0)\n",
            "TESTING: Loss: 0.5727861507942802 | Acc: 0.7149837133550488 \t\n",
            "(0.6421267893660532, 1.0)\n",
            "loss: 0.013832554552290174\tacc: 0.6979020979020979\n",
            "(0.6282271944922547, 1.0)\n",
            "TESTING: Loss: 0.5646498454244513 | Acc: 0.7149837133550488 \t\n",
            "(0.6421267893660532, 1.0)\n",
            "loss: 0.013917276594373916\tacc: 0.7027972027972028\n",
            "(0.6320346320346321, 1.0)\n",
            "TESTING: Loss: 0.5610374212265015 | Acc: 0.7149837133550488 \t\n",
            "(0.6421267893660532, 1.0)\n",
            "loss: 0.014522239896986219\tacc: 0.6993006993006993\n",
            "(0.6293103448275862, 1.0)\n",
            "TESTING: Loss: 0.5571536437461251 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.013381695747375489\tacc: 0.7034965034965035\n",
            "(0.6325823223570191, 1.0)\n",
            "TESTING: Loss: 0.553356796503067 | Acc: 0.7182410423452769 \t\n",
            "(0.6447638603696099, 1.0)\n",
            "loss: 0.013835272524091932\tacc: 0.7020979020979021\n",
            "(0.6314878892733564, 1.0)\n",
            "TESTING: Loss: 0.5514284607611204 | Acc: 0.7182410423452769 \t\n",
            "(0.6447638603696099, 1.0)\n",
            "loss: 0.009192356798383925\tacc: 0.7034965034965035\n",
            "(0.6325823223570191, 1.0)\n",
            "TESTING: Loss: 0.5512145240055887 | Acc: 0.7166123778501629 \t\n",
            "(0.6434426229508197, 1.0)\n",
            "loss: 0.013616381751166449\tacc: 0.7034965034965035\n",
            "(0.6325823223570191, 1.0)\n",
            "TESTING: Loss: 0.5487275045169028 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.015764576858944362\tacc: 0.7027972027972028\n",
            "(0.6320346320346321, 1.0)\n",
            "TESTING: Loss: 0.5477526736886877 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.012752430968814427\tacc: 0.7041958041958042\n",
            "(0.6331309627059843, 1.0)\n",
            "TESTING: Loss: 0.5469935724609777 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.010478555493884616\tacc: 0.7055944055944056\n",
            "(0.634231103388358, 1.0)\n",
            "TESTING: Loss: 0.5467893324400249 | Acc: 0.7182410423452769 \t\n",
            "(0.6447638603696099, 1.0)\n",
            "loss: 0.010630915562311808\tacc: 0.7041958041958042\n",
            "(0.6331309627059843, 1.0)\n",
            "TESTING: Loss: 0.5460079892685539 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.011270267433590359\tacc: 0.7055944055944056\n",
            "(0.634231103388358, 1.0)\n",
            "TESTING: Loss: 0.5459807185750258 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.012625432014465332\tacc: 0.7048951048951049\n",
            "(0.6336805555555556, 1.0)\n",
            "TESTING: Loss: 0.5451701487365522 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "loss: 0.012570452690124512\tacc: 0.7041958041958042\n",
            "(0.6331309627059843, 1.0)\n",
            "TESTING: Loss: 0.5452133856321636 | Acc: 0.7247557003257329 \t\n",
            "(0.650103519668737, 1.0)\n",
            "loss: 0.009310052792231242\tacc: 0.7062937062937062\n",
            "(0.6347826086956522, 1.0)\n",
            "TESTING: Loss: 0.5449376561139759 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "********** \tTraing new model on class: 7 \t **********\n",
            "loss: 0.01383482732556083\tacc: 0.6133428981348638\n",
            "(0.5628548256285483, 1.0)\n",
            "TESTING: Loss: 0.6834243304199643 | Acc: 0.6989966555183946 \t\n",
            "(0.6234309623430963, 1.0)\n",
            "loss: 0.015223737467419018\tacc: 0.7166427546628408\n",
            "(0.6372819100091828, 1.0)\n",
            "TESTING: Loss: 0.640670379002889 | Acc: 0.7090301003344481 \t\n",
            "(0.6313559322033898, 1.0)\n",
            "loss: 0.011993782086805864\tacc: 0.7216642754662841\n",
            "(0.6414048059149723, 1.0)\n",
            "TESTING: Loss: 0.6057354973422157 | Acc: 0.7190635451505016 \t\n",
            "(0.6394849785407726, 1.0)\n",
            "loss: 0.013607953082431446\tacc: 0.7216642754662841\n",
            "(0.6414048059149723, 1.0)\n",
            "TESTING: Loss: 0.580954275197453 | Acc: 0.7207357859531772 \t\n",
            "(0.6408602150537634, 1.0)\n",
            "loss: 0.009925361384044994\tacc: 0.7252510760401721\n",
            "(0.6443825441039925, 1.0)\n",
            "TESTING: Loss: 0.5662388636006249 | Acc: 0.7207357859531772 \t\n",
            "(0.6408602150537634, 1.0)\n",
            "loss: 0.01082141561941667\tacc: 0.7245337159253945\n",
            "(0.6437847866419295, 1.0)\n",
            "TESTING: Loss: 0.5565130495362811 | Acc: 0.7207357859531772 \t\n",
            "(0.6408602150537634, 1.0)\n",
            "loss: 0.009806538847359743\tacc: 0.7259684361549498\n",
            "(0.6449814126394052, 1.0)\n",
            "TESTING: Loss: 0.5494959039820565 | Acc: 0.725752508361204 \t\n",
            "(0.645021645021645, 1.0)\n",
            "loss: 0.012750694697553461\tacc: 0.7288378766140603\n",
            "(0.6473880597014925, 1.0)\n",
            "TESTING: Loss: 0.5454323838154475 | Acc: 0.7240802675585284 \t\n",
            "(0.6436285097192225, 1.0)\n",
            "loss: 0.013792533766139637\tacc: 0.7281205164992827\n",
            "(0.646784715750233, 1.0)\n",
            "TESTING: Loss: 0.5442773898442587 | Acc: 0.7240802675585284 \t\n",
            "(0.6436285097192225, 1.0)\n",
            "loss: 0.009166427633979103\tacc: 0.7281205164992827\n",
            "(0.646784715750233, 1.0)\n",
            "TESTING: Loss: 0.5409211814403534 | Acc: 0.725752508361204 \t\n",
            "(0.645021645021645, 1.0)\n",
            "loss: 0.010925602506507526\tacc: 0.7281205164992827\n",
            "(0.646784715750233, 1.0)\n",
            "TESTING: Loss: 0.5385384443733428 | Acc: 0.7290969899665551 \t\n",
            "(0.6478260869565218, 1.0)\n",
            "loss: 0.006308991800655018\tacc: 0.7302725968436155\n",
            "(0.6485981308411215, 1.0)\n",
            "TESTING: Loss: 0.5374529427952237 | Acc: 0.7324414715719063 \t\n",
            "(0.6506550218340611, 1.0)\n",
            "loss: 0.012587638063864275\tacc: 0.7309899569583931\n",
            "(0.6492048643592142, 1.0)\n",
            "TESTING: Loss: 0.5368913263082504 | Acc: 0.7274247491638796 \t\n",
            "(0.6464208242950108, 1.0)\n",
            "loss: 0.011577909642999823\tacc: 0.7302725968436155\n",
            "(0.6485981308411215, 1.0)\n",
            "TESTING: Loss: 0.5352854844596651 | Acc: 0.7290969899665551 \t\n",
            "(0.6478260869565218, 1.0)\n",
            "loss: 0.010638495737856085\tacc: 0.7309899569583931\n",
            "(0.6492048643592142, 1.0)\n",
            "TESTING: Loss: 0.5341389212343428 | Acc: 0.7324414715719063 \t\n",
            "(0.6506550218340611, 1.0)\n",
            "loss: 0.011120816523378546\tacc: 0.7309899569583931\n",
            "(0.6492048643592142, 1.0)\n",
            "TESTING: Loss: 0.5342152333921857 | Acc: 0.7290969899665551 \t\n",
            "(0.6478260869565218, 1.0)\n",
            "loss: 0.01139553568579934\tacc: 0.7317073170731707\n",
            "(0.649812734082397, 1.0)\n",
            "TESTING: Loss: 0.5325787398550246 | Acc: 0.7324414715719063 \t\n",
            "(0.6506550218340611, 1.0)\n",
            "loss: 0.008881268176165495\tacc: 0.7317073170731707\n",
            "(0.649812734082397, 1.0)\n",
            "TESTING: Loss: 0.5327263441350725 | Acc: 0.7290969899665551 \t\n",
            "(0.6478260869565218, 1.0)\n",
            "loss: 0.0101829624988816\tacc: 0.7317073170731707\n",
            "(0.649812734082397, 1.0)\n",
            "TESTING: Loss: 0.5314652224381765 | Acc: 0.7324414715719063 \t\n",
            "(0.6506550218340611, 1.0)\n",
            "loss: 0.010416444052349438\tacc: 0.7317073170731707\n",
            "(0.649812734082397, 1.0)\n",
            "TESTING: Loss: 0.5323899537324905 | Acc: 0.7290969899665551 \t\n",
            "(0.6478260869565218, 1.0)\n",
            "********** \tTraing new model on class: 8 \t **********\n",
            "loss: 0.015599474642011854\tacc: 0.5677009873060649\n",
            "(0.5525525525525525, 0.7688022284122563)\n",
            "TESTING: Loss: 0.6944607276665536 | Acc: 0.6732348111658456 \t\n",
            "(0.6082677165354331, 1.0)\n",
            "loss: 0.014578567610846626\tacc: 0.6904090267983075\n",
            "(0.6205704407951599, 1.0)\n",
            "TESTING: Loss: 0.6540352513915614 | Acc: 0.6847290640394089 \t\n",
            "(0.6167664670658682, 1.0)\n",
            "loss: 0.016486263275146483\tacc: 0.6974612129760226\n",
            "(0.6259808195292066, 1.0)\n",
            "TESTING: Loss: 0.6183864678207197 | Acc: 0.6896551724137931 \t\n",
            "(0.6204819277108434, 1.0)\n",
            "loss: 0.011788071526421442\tacc: 0.6995768688293371\n",
            "(0.6276223776223776, 1.0)\n",
            "TESTING: Loss: 0.5906767954951838 | Acc: 0.6945812807881774 \t\n",
            "(0.6242424242424243, 1.0)\n",
            "loss: 0.017444752322302925\tacc: 0.7023977433004231\n",
            "(0.6298245614035087, 1.0)\n",
            "TESTING: Loss: 0.5724138601830131 | Acc: 0.6945812807881774 \t\n",
            "(0.6242424242424243, 1.0)\n",
            "loss: 0.009407507710986668\tacc: 0.7023977433004231\n",
            "(0.6298245614035087, 1.0)\n",
            "TESTING: Loss: 0.5623090016214471 | Acc: 0.6945812807881774 \t\n",
            "(0.6242424242424243, 1.0)\n",
            "loss: 0.010124114486906264\tacc: 0.7023977433004231\n",
            "(0.6298245614035087, 1.0)\n",
            "TESTING: Loss: 0.5538806758428875 | Acc: 0.6995073891625616 \t\n",
            "(0.6280487804878049, 1.0)\n",
            "loss: 0.013427656226687961\tacc: 0.7023977433004231\n",
            "(0.6298245614035087, 1.0)\n",
            "TESTING: Loss: 0.5490512818490204 | Acc: 0.7011494252873564 \t\n",
            "(0.6293279022403259, 1.0)\n",
            "loss: 0.011744066079457601\tacc: 0.7031029619181947\n",
            "(0.630377524143986, 1.0)\n",
            "TESTING: Loss: 0.5456567719382676 | Acc: 0.7044334975369458 \t\n",
            "(0.6319018404907976, 1.0)\n",
            "loss: 0.008086276054382325\tacc: 0.7031029619181947\n",
            "(0.630377524143986, 1.0)\n",
            "TESTING: Loss: 0.5431884930125976 | Acc: 0.7060755336617406 \t\n",
            "(0.6331967213114754, 1.0)\n",
            "loss: 0.01061275667614407\tacc: 0.7031029619181947\n",
            "(0.630377524143986, 1.0)\n",
            "TESTING: Loss: 0.5413581551493782 | Acc: 0.7060755336617406 \t\n",
            "(0.6331967213114754, 1.0)\n",
            "loss: 0.010104917155371772\tacc: 0.7045133991537377\n",
            "(0.6314863676341249, 1.0)\n",
            "TESTING: Loss: 0.5413011701189374 | Acc: 0.7027914614121511 \t\n",
            "(0.6306122448979592, 1.0)\n",
            "loss: 0.010930309030744765\tacc: 0.7045133991537377\n",
            "(0.6314863676341249, 1.0)\n",
            "TESTING: Loss: 0.5387851100316957 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "loss: 0.011042890283796522\tacc: 0.7094499294781382\n",
            "(0.6353982300884956, 1.0)\n",
            "TESTING: Loss: 0.537377496813669 | Acc: 0.7060755336617406 \t\n",
            "(0.6331967213114754, 1.0)\n",
            "loss: 0.0100654317273034\tacc: 0.7045133991537377\n",
            "(0.6314863676341249, 1.0)\n",
            "TESTING: Loss: 0.5363629850755004 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "loss: 0.016873462994893392\tacc: 0.7052186177715092\n",
            "(0.6320422535211268, 1.0)\n",
            "TESTING: Loss: 0.535468718478162 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "loss: 0.013680489857991536\tacc: 0.7073342736248237\n",
            "(0.6337157987643425, 1.0)\n",
            "TESTING: Loss: 0.5352062689091422 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "loss: 0.009249918990665011\tacc: 0.7080394922425952\n",
            "(0.6342756183745583, 1.0)\n",
            "TESTING: Loss: 0.5341086653919008 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "loss: 0.009683648082945083\tacc: 0.7101551480959097\n",
            "(0.6359610274579274, 1.0)\n",
            "TESTING: Loss: 0.5343859993460539 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "loss: 0.015121274524264865\tacc: 0.7059238363892807\n",
            "(0.6325991189427312, 1.0)\n",
            "TESTING: Loss: 0.533166448683723 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "********** \tTraing new model on class: 9 \t **********\n",
            "loss: 0.01492093052974967\tacc: 0.6351744186046512\n",
            "(0.5738539898132428, 1.0)\n",
            "TESTING: Loss: 0.6920897364616394 | Acc: 0.6813559322033899 \t\n",
            "(0.606694560669456, 1.0)\n",
            "loss: 0.013905216095059417\tacc: 0.7034883720930233\n",
            "(0.6236162361623616, 1.0)\n",
            "TESTING: Loss: 0.6518084638648562 | Acc: 0.7050847457627119 \t\n",
            "(0.625, 1.0)\n",
            "loss: 0.013932571854702262\tacc: 0.7100290697674418\n",
            "(0.6288372093023256, 1.0)\n",
            "TESTING: Loss: 0.6163831949234009 | Acc: 0.7101694915254237 \t\n",
            "(0.6290672451193059, 1.0)\n",
            "loss: 0.012891622476799543\tacc: 0.715843023255814\n",
            "(0.633552014995314, 1.0)\n",
            "TESTING: Loss: 0.5917380270030763 | Acc: 0.7135593220338983 \t\n",
            "(0.6318082788671024, 1.0)\n",
            "loss: 0.012844028861023659\tacc: 0.715843023255814\n",
            "(0.633552014995314, 1.0)\n",
            "TESTING: Loss: 0.574969114528762 | Acc: 0.7169491525423729 \t\n",
            "(0.6345733041575492, 1.0)\n",
            "loss: 0.010942364847937296\tacc: 0.7180232558139535\n",
            "(0.6353383458646616, 1.0)\n",
            "TESTING: Loss: 0.5650086618132062 | Acc: 0.7169491525423729 \t\n",
            "(0.6345733041575492, 1.0)\n",
            "loss: 0.013901429120884386\tacc: 0.71875\n",
            "(0.6359360301034808, 1.0)\n",
            "TESTING: Loss: 0.5594597491953108 | Acc: 0.7169491525423729 \t\n",
            "(0.6345733041575492, 1.0)\n",
            "loss: 0.010034210460130559\tacc: 0.7194767441860465\n",
            "(0.6365348399246704, 1.0)\n",
            "TESTING: Loss: 0.553947024875217 | Acc: 0.7186440677966102 \t\n",
            "(0.6359649122807017, 1.0)\n",
            "loss: 0.01143422099046929\tacc: 0.7209302325581395\n",
            "(0.6377358490566037, 1.0)\n",
            "TESTING: Loss: 0.5530466106202867 | Acc: 0.7169491525423729 \t\n",
            "(0.6345733041575492, 1.0)\n",
            "loss: 0.011706799961799798\tacc: 0.7194767441860465\n",
            "(0.6365348399246704, 1.0)\n",
            "TESTING: Loss: 0.5493558529350493 | Acc: 0.7186440677966102 \t\n",
            "(0.6359649122807017, 1.0)\n",
            "loss: 0.010671032722606215\tacc: 0.7194767441860465\n",
            "(0.6365348399246704, 1.0)\n",
            "TESTING: Loss: 0.5469619731108347 | Acc: 0.7203389830508474 \t\n",
            "(0.6373626373626373, 1.0)\n",
            "loss: 0.010040085676104524\tacc: 0.7238372093023255\n",
            "(0.6401515151515151, 1.0)\n",
            "TESTING: Loss: 0.5460660407940546 | Acc: 0.7203389830508474 \t\n",
            "(0.6373626373626373, 1.0)\n",
            "loss: 0.010489408359971157\tacc: 0.7223837209302325\n",
            "(0.6389413988657845, 1.0)\n",
            "TESTING: Loss: 0.5449594590399001 | Acc: 0.7203389830508474 \t\n",
            "(0.6373626373626373, 1.0)\n",
            "loss: 0.012771261292834615\tacc: 0.721656976744186\n",
            "(0.6383380547686497, 1.0)\n",
            "TESTING: Loss: 0.5436848998069763 | Acc: 0.7203389830508474 \t\n",
            "(0.6373626373626373, 1.0)\n",
            "loss: 0.012252219887666924\tacc: 0.7238372093023255\n",
            "(0.6401515151515151, 1.0)\n",
            "TESTING: Loss: 0.5444756928417418 | Acc: 0.7203389830508474 \t\n",
            "(0.6373626373626373, 1.0)\n",
            "loss: 0.011066382707551468\tacc: 0.7231104651162791\n",
            "(0.6395458845789972, 1.0)\n",
            "TESTING: Loss: 0.5420226636860106 | Acc: 0.7220338983050848 \t\n",
            "(0.6387665198237885, 1.0)\n",
            "loss: 0.012481707473133886\tacc: 0.7245639534883721\n",
            "(0.6407582938388625, 1.0)\n",
            "TESTING: Loss: 0.5414304120673074 | Acc: 0.7220338983050848 \t\n",
            "(0.6387665198237885, 1.0)\n",
            "loss: 0.012316681617914244\tacc: 0.7252906976744186\n",
            "(0.6413662239089184, 1.0)\n",
            "TESTING: Loss: 0.5417533914248148 | Acc: 0.7203389830508474 \t\n",
            "(0.6373626373626373, 1.0)\n",
            "loss: 0.009890399699987368\tacc: 0.7245639534883721\n",
            "(0.6407582938388625, 1.0)\n",
            "TESTING: Loss: 0.5402922613753213 | Acc: 0.7237288135593221 \t\n",
            "(0.6401766004415012, 1.0)\n",
            "loss: 0.01304858784342921\tacc: 0.7260174418604651\n",
            "(0.6419753086419753, 1.0)\n",
            "TESTING: Loss: 0.5401116410891215 | Acc: 0.7220338983050848 \t\n",
            "(0.6387665198237885, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce1ef0>\n",
            "TESTING: Loss: 0.5517809357908037 | Acc: 0.714759535655058 \t\n",
            "(0.6378947368421053, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce1a58>\n",
            "TESTING: Loss: 0.5443261480993695 | Acc: 0.7220394736842105 \t\n",
            "(0.6457023060796646, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce1ba8>\n",
            "TESTING: Loss: 0.4110036740700404 | Acc: 0.8200339558573854 \t\n",
            "(0.7316455696202532, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce1b38>\n",
            "TESTING: Loss: 0.3903703313124807 | Acc: 0.8491803278688524 \t\n",
            "(0.7711442786069652, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce00f0>\n",
            "TESTING: Loss: 0.4334261119365692 | Acc: 0.7914529914529914 \t\n",
            "(0.7002457002457002, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce0550>\n",
            "TESTING: Loss: 0.38785069270266426 | Acc: 0.8363939899833055 \t\n",
            "(0.7531486146095718, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce01d0>\n",
            "TESTING: Loss: 0.5449376561139759 | Acc: 0.7198697068403909 \t\n",
            "(0.6460905349794238, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce05c0>\n",
            "TESTING: Loss: 0.5323899537324905 | Acc: 0.7290969899665551 \t\n",
            "(0.6478260869565218, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce0320>\n",
            "TESTING: Loss: 0.533166448683723 | Acc: 0.7077175697865353 \t\n",
            "(0.6344969199178645, 1.0)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f7393ce0748>\n",
            "TESTING: Loss: 0.5401116410891215 | Acc: 0.7220338983050848 \t\n",
            "(0.6387665198237885, 1.0)\n",
            "TESTING precision: 0.6768018018018018, recall: 1.0\n",
            "confusion matrix:\n",
            "TN: 1565\tFP: 1435\n",
            "FN: 0\t TP:3005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wVoZBgabiaf",
        "colab_type": "text"
      },
      "source": [
        "# Randomize posterior by a CNN trainined on testing dataset\n",
        "\n",
        "defender net are the same as target model but trained on cifar-10 testing dataset.\n",
        "if the attacker feed membership data of target mode, the output is defender's posteriors.\n",
        "Otherwise, output is target model's posteriors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCq7OXRyjGDe",
        "colab_type": "code",
        "outputId": "f7cb65e3-c3de-474c-a154-26f672e2bfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "# defender_x, defender_y, defender_c\n",
        "\n",
        "print('non-membership number:', defender_y.eq(0).sum().item())\n",
        "\n",
        "defenderDs = {}\n",
        "defenderDls = {}\n",
        "num_classes = 10\n",
        "for i in range(0, num_classes):\n",
        "  class_i = defender_c.eq(i).nonzero().squeeze(dim=1)\n",
        "  defenderDs[i] = TensorDataset(defender_x[class_i], defender_y[class_i])\n",
        "  defenderDls[i] =  torch.utils.data.DataLoader(defenderDs[i], batch_size=bz, shuffle=False)\n",
        "\n",
        "\n",
        "mab.test_all_classes(class_num=10, dataloader_dic=defenderDls)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "non-membership number: 0\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f021791bf28>\n",
            "TESTING: Loss: 6.679849670779321 | Acc: 0.457 \t\n",
            "(1.0, 0.457)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0217926048>\n",
            "TESTING: Loss: 8.064151640861265 | Acc: 0.530791788856305 \t\n",
            "(1.0, 0.530791788856305)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0217926128>\n",
            "TESTING: Loss: 7.0921243544547785 | Acc: 0.34833659491193736 \t\n",
            "(1.0, 0.34833659491193736)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0217926208>\n",
            "TESTING: Loss: 10.49216882387797 | Acc: 0.2595959595959596 \t\n",
            "(1.0, 0.2595959595959596)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179262e8>\n",
            "TESTING: Loss: 7.62769668896993 | Acc: 0.3763874873864783 \t\n",
            "(1.0, 0.3763874873864783)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179263c8>\n",
            "TESTING: Loss: 9.849604206700478 | Acc: 0.32245301681503463 \t\n",
            "(1.0, 0.32245301681503463)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f02179264a8>\n",
            "TESTING: Loss: 8.739103409551806 | Acc: 0.47960199004975124 \t\n",
            "(1.0, 0.47960199004975124)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0217926588>\n",
            "TESTING: Loss: 7.279148785273234 | Acc: 0.47653061224489796 \t\n",
            "(1.0, 0.47653061224489796)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0217926668>\n",
            "TESTING: Loss: 9.93318723863171 | Acc: 0.5400801603206413 \t\n",
            "(1.0, 0.5400801603206413)\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f0217926748>\n",
            "TESTING: Loss: 9.248117176691691 | Acc: 0.4785714285714286 \t\n",
            "(1.0, 0.4785714285714286)\n",
            "TESTING precision: 1.0, recall: 0.4269\n",
            "confusion matrix:\n",
            "TN: 0\tFP: 0\n",
            "FN: 5731\t TP:4269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}